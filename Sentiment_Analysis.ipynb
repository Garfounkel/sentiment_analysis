{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "This notebook is the main one for this project. It will help navigate through the implementation scripts and notebooks, without going into too much detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Embeddings\n",
    "\n",
    "The first part of this project was to find a big enough dataset. As mantioned in the notebook *\"src/word2vec_training\"*, different techniques were investigated. We finally decided to pick this dataset:\n",
    "\n",
    "https://archive.org/details/archiveteam-twitter-stream-2017-11\n",
    "\n",
    "From this base we filtered non english, truncated, retweeted or duplicate tweets. You can download the resulting dataset of 23M tweets we used here:\n",
    "\n",
    "https://mega.nz/#!UI0ViKiZ!x6eBjFPmkKqDcV6Il-rpQj-DNcSJIOeL6Axk-vfuOyU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a preprocessing pipeline example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from src.text_preprocessing import TweetTokenizer, NLTKStemmer, NLTKLemmatizer, CorpusWrapper\n",
    "\n",
    "input_stream = [\n",
    "    'This is a tweet',\n",
    "    'This is another tweet',\n",
    "]\n",
    "\n",
    "factories = [\n",
    "    TweetTokenizer,\n",
    "    partial(CorpusWrapper, NLTKStemmer),\n",
    "    partial(CorpusWrapper, NLTKLemmatizer),\n",
    "    partial(BatchMaker, batch_size=100000),\n",
    "]\n",
    "\n",
    "batch_pipeline = Pipeline(input_stream, factories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'be', 'a', 'tweet']\n",
      "['thi', 'be', 'anoth', 'tweet']\n"
     ]
    }
   ],
   "source": [
    "for batch in batch_pipeline:\n",
    "    for tweet in batch:\n",
    "        print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used Gensim to train embeddings on our dataset, using a previously defined pipeline. You can load our embeddings this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec.load('./data/trained_embeddings_23M.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, after testing, we noticed that the model for 3 classes was more performent with pre-trained embeddings on more than 330M tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "\n",
    "tmp_file = get_tmpfile('datastories.300d.word2vec')\n",
    "glove2word2vec('./data/embeddings/datastories.twitter.300d.txt', tmp_file)\n",
    "w2v = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emolex\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 class training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_model(model):\n",
    "    model.pop()\n",
    "    model.pop()\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    dense1 = Dense(150, activation='relu')\n",
    "    dense3 = Dense(80, activation='relu')\n",
    "    dense4 = Dense(30, activation='relu')\n",
    "    dense2 = Dense(7, activation='softmax')\n",
    "\n",
    "    model.add(dense1)\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(dense3)\n",
    "    model.add(dense4)\n",
    "    model.add(Flatten())\n",
    "    model.add(dense2)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Emolex\n",
    "  - improve this list\n",
    "  - load the embedding matrix\n",
    "- 3 class training\n",
    "  - improve this list\n",
    "  - show the Keras model\n",
    "  - load the trained model\n",
    "- Transfer learning\n",
    "  - show the transformation function\n",
    "  - model summary\n",
    "  - show accuracy and pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
