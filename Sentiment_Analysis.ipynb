{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "This notebook is the main one for this project. It will help navigate through the implementation scripts and notebooks, without going into too much detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:51:25.991787Z",
     "start_time": "2018-11-29T17:51:25.971930Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import *\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizer import tokenizer as tweet_tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import *\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import * \n",
    "from keras.regularizers import *\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "import json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from src.preprocessing import standardization\n",
    "import os\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "PATH = './data'\n",
    "\n",
    "path_train_3 = f'{PATH}/data_train_3.csv'\n",
    "path_test_3 = f'{PATH}/data_test_3.csv'\n",
    "path_train_7 = f'{PATH}/data_train_7.csv'\n",
    "path_val_7 = f'{PATH}/data_val_7.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Embeddings\n",
    "\n",
    "The first part of this project was to find a big enough dataset. As mantioned in the notebook *\"src/word2vec_training\"*, different techniques were investigated. We finally decided to pick this dataset:\n",
    "\n",
    "https://archive.org/details/archiveteam-twitter-stream-2017-11\n",
    "\n",
    "From this base we filtered non english, truncated, retweeted or duplicate tweets. You can download the resulting dataset of 23M tweets we used here:\n",
    "\n",
    "https://mega.nz/#!UI0ViKiZ!x6eBjFPmkKqDcV6Il-rpQj-DNcSJIOeL6Axk-vfuOyU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a preprocessing pipeline example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:04:59.931360Z",
     "start_time": "2018-11-29T17:04:59.919861Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from src.text_preprocessing import TweetTokenizer, NLTKStemmer, NLTKLemmatizer, CorpusWrapper, BatchMaker, Pipeline\n",
    "\n",
    "input_stream = [\n",
    "    'This is a tweet',\n",
    "    'This is another tweet',\n",
    "]\n",
    "\n",
    "factories = [\n",
    "    TweetTokenizer,\n",
    "    partial(CorpusWrapper, NLTKStemmer),\n",
    "    partial(CorpusWrapper, NLTKLemmatizer),\n",
    "    partial(BatchMaker, batch_size=100000),\n",
    "]\n",
    "\n",
    "batch_pipeline = Pipeline(input_stream, factories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:05:00.668246Z",
     "start_time": "2018-11-29T17:05:00.656958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'be', 'a', 'tweet']\n",
      "['thi', 'be', 'anoth', 'tweet']\n"
     ]
    }
   ],
   "source": [
    "for batch in batch_pipeline:\n",
    "    for tweet in batch:\n",
    "        print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing our datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:25:44.237100Z",
     "start_time": "2018-11-29T17:25:43.978414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50333, 3), (1630, 3), (1630, 3))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(path_train_3, sep='\\t', names=['ID', 'Class', 'Tweet'])\n",
    "tweets_7 = pd.read_csv(path_train_7, sep='\\t', names=['ID', 'Class', 'Tweet'], dtype={'Tweet': str})\n",
    "tweets_3_test = pd.read_csv(path_test_3, sep='\\t', names=['ID', 'Class', 'Tweet'], dtype={'Tweet': str})\n",
    "\n",
    "tweets.shape, tweets_7.shape, tweets_3_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:27:10.467322Z",
     "start_time": "2018-11-29T17:25:55.647330Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets['Sentiment'] = tweets['Class'].apply(lambda x: {'negative': 0, 'neutral': 1, 'positive': 2}[x])\n",
    "tweets['Tweet'] = tweets['Tweet'].apply(lambda x: standardization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:27:12.635371Z",
     "start_time": "2018-11-29T17:27:10.474282Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_7['Sentiment'] = tweets_7['Class'] + 3\n",
    "tweets_7['Tweet'] = tweets_7['Tweet'].apply(lambda x: standardization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:27:14.581353Z",
     "start_time": "2018-11-29T17:27:12.638582Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_3_test['Sentiment'] = tweets_3_test['Class'].apply(lambda x: {'negative': 0, 'neutral': 1, 'positive': 2}[x])\n",
    "tweets_3_test['Tweet'] = tweets_3_test['Tweet'].apply(lambda x: standardization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:27:14.624184Z",
     "start_time": "2018-11-29T17:27:14.584813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11552</th>\n",
       "      <td>281269284217421824</td>\n",
       "      <td>positive</td>\n",
       "      <td>watch nightmare christmas first time long time...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18098</th>\n",
       "      <td>634203696872579072</td>\n",
       "      <td>positive</td>\n",
       "      <td>snoop dogg gonna centennial game friday gonna ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID     Class  \\\n",
       "11552  281269284217421824  positive   \n",
       "18098  634203696872579072  positive   \n",
       "\n",
       "                                                   Tweet  Sentiment  \n",
       "11552  watch nightmare christmas first time long time...          2  \n",
       "18098  snoop dogg gonna centennial game friday gonna ...          2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yeah ☺ ️ playing well</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>least not guy try discourage anymore want neve...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Class                                              Tweet  Sentiment\n",
       "0   0      0                              yeah ☺ ️ playing well          3\n",
       "1   1      0  least not guy try discourage anymore want neve...          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>449</td>\n",
       "      <td>negative</td>\n",
       "      <td>site crash everytime try book help tell nothin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>450</td>\n",
       "      <td>negative</td>\n",
       "      <td>theme week ask lord strength perspective perse...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID     Class                                              Tweet  Sentiment\n",
       "0  449  negative  site crash everytime try book help tell nothin...          0\n",
       "1  450  negative  theme week ask lord strength perspective perse...          0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tweets.sample(2))\n",
    "display(tweets_7.head(2))\n",
    "display(tweets_3_test.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:29:07.941156Z",
     "start_time": "2018-11-29T17:29:06.896136Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x, train_y = tweets['Tweet'], tweets['Sentiment']\n",
    "test_x, test_y = tweets_3_test['Tweet'], tweets_3_test['Sentiment']\n",
    "train7_x, train7_y = tweets_7['Tweet'], tweets_7['Sentiment']\n",
    "\n",
    "all_tweets = pd.concat([train_x, test_x, train7_x])\n",
    "tokenizer = Tokenizer(filters=' ')\n",
    "tokenizer.fit_on_texts(all_tweets)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used Gensim to train embeddings on our dataset, using a previously defined pipeline. You can load our embeddings this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:05:08.879477Z",
     "start_time": "2018-11-29T17:05:08.222111Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# model = Word2Vec.load('./data/trained_embeddings_23M.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, after testing, we noticed that the model for 3 classes was more performent with pre-trained embeddings on more than 330M tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:16:23.617524Z",
     "start_time": "2018-11-29T17:12:53.702532Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "\n",
    "tmp_file = get_tmpfile('datastories.300d.word2vec')\n",
    "glove2word2vec('./data/embeddings/datastories.twitter.300d.txt', tmp_file)\n",
    "w2v = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enriching the embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmoLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:16:23.709173Z",
     "start_time": "2018-11-29T17:16:23.636113Z"
    }
   },
   "outputs": [],
   "source": [
    "emolex = pd.read_csv('data/EmoLex.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:16:23.730099Z",
     "start_time": "2018-11-29T17:16:23.713804Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_words = []\n",
    "negative_words = []\n",
    "\n",
    "with open('data/positive-words.txt') as positive_file, open('data/negative-words.txt', encoding='ISO-8859-1') as negative_file:\n",
    "    for _ in range(35):\n",
    "        next(positive_file)\n",
    "        next(negative_file)\n",
    "        \n",
    "    for line in positive_file:\n",
    "        positive_words.append(line)\n",
    "    for line in negative_file:\n",
    "        negative_words.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji valence and AFINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:16:23.876939Z",
     "start_time": "2018-11-29T17:16:23.826195Z"
    }
   },
   "outputs": [],
   "source": [
    "afinn = pd.read_csv('data/AFINN-111.txt', sep='\\t')\n",
    "\n",
    "def val_to_list(x):\n",
    "    x += 5\n",
    "    return(to_categorical(x, num_classes=11, dtype='int'))\n",
    "\n",
    "\n",
    "afinn[['val']] = afinn['val'].apply(val_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:16:23.905525Z",
     "start_time": "2018-11-29T17:16:23.893695Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./data/index.json') as emojiFile:\n",
    "    emoji_valence = json.load(emojiFile)\n",
    "\n",
    "for elmt in emoji_valence:\n",
    "    val = elmt['polarity']\n",
    "    elmt['polarity'] = val_to_list(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depeche Mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:16:24.679161Z",
     "start_time": "2018-11-29T17:16:23.908814Z"
    }
   },
   "outputs": [],
   "source": [
    "DepecheMoodpp = pd.read_csv('data/DepecheMood/DepecheMood_english_token_full.tsv', sep='\\t')\n",
    "DepecheMood = pd.read_csv('data/DepecheMood/DepecheMood_freq.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:39:13.607126Z",
     "start_time": "2018-11-29T17:29:45.676360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37785, 342)\n"
     ]
    }
   ],
   "source": [
    "nb_words = len(word_index) + 1\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "EMOLEX_DIM = 10\n",
    "OLE_DIM = 2\n",
    "EMOJI_VALENCE_DIM = 11\n",
    "AFINN_DIM = 11 \n",
    "DEPECHE_MOOD_DIM = 8\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM + EMOLEX_DIM + OLE_DIM + AFINN_DIM + DEPECHE_MOOD_DIM + EMOJI_VALENCE_DIM))\n",
    "\n",
    "oov = []  # Out of vocabulary\n",
    "oov.append((np.random.rand(EMBEDDING_DIM) * 2.0) - 1.0)\n",
    "oov = oov / np.linalg.norm(oov)\n",
    "empty_afinn = np.full(11, 0)\n",
    "empty_emoji = np.full(11, 0)\n",
    "empty_emolex = np.full(10, 0)\n",
    "empty_depeche = np.full(8, 0)\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    emoji_val = 0\n",
    "    \n",
    "    word_vector = oov\n",
    "    if word in w2v.vocab:\n",
    "        word_vector = w2v.word_vec(word)\n",
    "\n",
    "    emolex_row = emolex.loc[emolex['word'] == word]\n",
    "    if emolex_row.empty:\n",
    "        word_vector = np.append(word_vector, empty_emolex)\n",
    "    else:\n",
    "        word_vector = np.append(word_vector, emolex_row.values.tolist()[0][1:])\n",
    "        \n",
    "    depeche_row = DepecheMoodpp.loc[DepecheMoodpp['word'] == word]\n",
    "    if depeche_row.empty:\n",
    "        word_vector = np.append(word_vector, empty_depeche)\n",
    "    else:\n",
    "        word_vector = np.append(word_vector, depeche_row.values.tolist()[0][1:9])\n",
    "        \n",
    "    ole_val = [0, 0]\n",
    "    if word in positive_words:\n",
    "        ole_val = [1, 0]\n",
    "    elif word in negative_words:\n",
    "        ole_val = [0, 1]\n",
    "    word_vector = np.append(word_vector, ole_val)\n",
    "    \n",
    "    afinn_val = empty_afinn\n",
    "    emoji_val = empty_emoji\n",
    "    \n",
    "    afinn_row = afinn.loc[afinn['word'] == word]\n",
    "    if not afinn_row.empty:\n",
    "        afinn_val = afinn_row['val'].item()\n",
    "    else:\n",
    "        for emoji in emoji_valence:\n",
    "            if word == emoji['emoji']:\n",
    "                emoji_val = emoji['polarity']\n",
    "\n",
    "    word_vector = np.append(word_vector, afinn_val)\n",
    "    \n",
    "    word_vector = np.append(word_vector, emoji_val)\n",
    "    \n",
    "    embedding_matrix[i] = word_vector\n",
    "\n",
    "        \n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T14:07:19.675209Z",
     "start_time": "2018-11-20T14:07:19.254848Z"
    }
   },
   "outputs": [],
   "source": [
    "# Claim memory back from this very large object we don't use anymore\n",
    "del w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source task (3 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:39:15.442246Z",
     "start_time": "2018-11-29T17:39:13.640218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50333, 32), (1630, 32), (1630, 32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "train7_sequences = tokenizer.texts_to_sequences(train7_x)\n",
    "\n",
    "sequences = train_sequences + test_sequences + train7_sequences\n",
    "MAX_SEQUENCE_LENGTH = 0\n",
    "for elt in sequences:\n",
    "    if len(elt) > MAX_SEQUENCE_LENGTH:\n",
    "        MAX_SEQUENCE_LENGTH = len(elt)\n",
    "\n",
    "train_sequences = pad_sequences(train_sequences, MAX_SEQUENCE_LENGTH)\n",
    "test_sequences = pad_sequences(test_sequences, MAX_SEQUENCE_LENGTH)\n",
    "train7_sequences = pad_sequences(train7_pad_sequencesuences, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_sequences.shape, test_sequences.shape, train7_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:39:15.477191Z",
     "start_time": "2018-11-29T17:39:15.445925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 35233 samples\n",
      "validation set: 15100 samples\n",
      "x_train: (35233, 32)\n",
      "y_train: (35233, 3)\n"
     ]
    }
   ],
   "source": [
    "targets = to_categorical(train_y, 3)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_sequences, targets, test_size=0.3)\n",
    "\n",
    "print('training set: ' + str(len(X_train)) + ' samples')\n",
    "print('validation set: ' + str(len(X_val)) + ' samples')\n",
    "\n",
    "print('x_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:39:15.487285Z",
     "start_time": "2018-11-29T17:39:15.480310Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_mine():\n",
    "    vocab_size = embedding_matrix.shape[0]\n",
    "    embedding_size = embedding_matrix.shape[1]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, weights=[embedding_matrix], \n",
    "                        input_length=MAX_SEQUENCE_LENGTH, trainable=False, name='embedding_layer'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:39:20.314453Z",
     "start_time": "2018-11-29T17:39:15.490556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 32, 342)           12922470  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 342)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 32, 300)           591600    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 32, 300)           541200    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32, 300)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 28803     \n",
      "=================================================================\n",
      "Total params: 14,084,073\n",
      "Trainable params: 1,161,603\n",
      "Non-trainable params: 12,922,470\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model_mine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:51:25.865196Z",
     "start_time": "2018-11-29T17:39:20.318943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35233 samples, validate on 15100 samples\n",
      "Epoch 1/6\n",
      "35233/35233 [==============================] - 126s 4ms/step - loss: 0.8394 - acc: 0.5918 - val_loss: 0.8304 - val_acc: 0.6094\n",
      "Epoch 2/6\n",
      "35233/35233 [==============================] - 120s 3ms/step - loss: 0.7681 - acc: 0.6391 - val_loss: 0.7444 - val_acc: 0.6548\n",
      "Epoch 3/6\n",
      "35233/35233 [==============================] - 122s 3ms/step - loss: 0.7425 - acc: 0.6536 - val_loss: 0.7273 - val_acc: 0.6616\n",
      "Epoch 4/6\n",
      "35233/35233 [==============================] - 120s 3ms/step - loss: 0.7160 - acc: 0.6710 - val_loss: 0.7367 - val_acc: 0.6574\n",
      "Epoch 5/6\n",
      "35233/35233 [==============================] - 121s 3ms/step - loss: 0.6968 - acc: 0.6804 - val_loss: 0.7131 - val_acc: 0.6721\n",
      "Epoch 6/6\n",
      "35233/35233 [==============================] - 111s 3ms/step - loss: 0.6730 - acc: 0.6973 - val_loss: 0.7481 - val_acc: 0.6594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc42400ecc0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, validation_data=(X_val, y_val), epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning (7 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:51:25.891763Z",
     "start_time": "2018-11-29T17:51:25.880947Z"
    }
   },
   "outputs": [],
   "source": [
    "def reshape_model(model):\n",
    "    model.pop()\n",
    "    model.pop()\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    dense1 = Dense(150, activation='relu')\n",
    "    dense3 = Dense(80, activation='relu')\n",
    "    dense4 = Dense(30, activation='relu')\n",
    "    dense2 = Dense(7, activation='softmax')\n",
    "\n",
    "    model.add(dense1)\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(dense3)\n",
    "    model.add(dense4)\n",
    "    model.add(Flatten())\n",
    "    model.add(dense2)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:53:12.941877Z",
     "start_time": "2018-11-29T17:53:12.554732Z"
    }
   },
   "outputs": [],
   "source": [
    "model = reshape_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:53:16.724082Z",
     "start_time": "2018-11-29T17:53:16.708922Z"
    }
   },
   "outputs": [],
   "source": [
    "targets = to_categorical(train7_y, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:53:17.316203Z",
     "start_time": "2018-11-29T17:53:17.298372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 1467 samples\n",
      "validation set: 163 samples\n",
      "x_train: (1467, 32)\n",
      "y_train: (1467, 7)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train7_sequences, targets, test_size=0.1)\n",
    "\n",
    "print('training set: ' + str(len(X_train)) + ' samples')\n",
    "print('validation set: ' + str(len(X_val)) + ' samples')\n",
    "\n",
    "print('x_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T18:06:45.648518Z",
     "start_time": "2018-11-29T18:05:29.835082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1467 samples, validate on 163 samples\n",
      "Epoch 1/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3689 - acc: 0.4628 - val_loss: 1.5238 - val_acc: 0.4110\n",
      "Epoch 2/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3535 - acc: 0.4772 - val_loss: 1.5044 - val_acc: 0.3742\n",
      "Epoch 3/30\n",
      "1467/1467 [==============================] - 2s 1ms/step - loss: 1.3547 - acc: 0.4581 - val_loss: 1.5394 - val_acc: 0.4049\n",
      "Epoch 4/30\n",
      "1467/1467 [==============================] - 2s 2ms/step - loss: 1.3578 - acc: 0.4669 - val_loss: 1.5432 - val_acc: 0.3926\n",
      "Epoch 5/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3541 - acc: 0.4676 - val_loss: 1.5284 - val_acc: 0.3681\n",
      "Epoch 6/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3474 - acc: 0.4697 - val_loss: 1.5594 - val_acc: 0.3436\n",
      "Epoch 7/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3373 - acc: 0.4826 - val_loss: 1.5355 - val_acc: 0.3681\n",
      "Epoch 8/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3493 - acc: 0.4594 - val_loss: 1.5509 - val_acc: 0.3558\n",
      "Epoch 9/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3356 - acc: 0.4738 - val_loss: 1.5516 - val_acc: 0.3988\n",
      "Epoch 10/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3431 - acc: 0.4683 - val_loss: 1.5371 - val_acc: 0.3804\n",
      "Epoch 11/30\n",
      "1467/1467 [==============================] - 2s 1ms/step - loss: 1.3399 - acc: 0.4683 - val_loss: 1.5335 - val_acc: 0.4049\n",
      "Epoch 12/30\n",
      "1467/1467 [==============================] - 2s 2ms/step - loss: 1.3278 - acc: 0.4833 - val_loss: 1.5493 - val_acc: 0.4110\n",
      "Epoch 13/30\n",
      "1467/1467 [==============================] - 2s 1ms/step - loss: 1.3349 - acc: 0.4724 - val_loss: 1.5360 - val_acc: 0.3374\n",
      "Epoch 14/30\n",
      "1467/1467 [==============================] - 2s 1ms/step - loss: 1.3436 - acc: 0.4792 - val_loss: 1.5424 - val_acc: 0.3865\n",
      "Epoch 15/30\n",
      "1467/1467 [==============================] - 2s 1ms/step - loss: 1.3392 - acc: 0.4628 - val_loss: 1.5419 - val_acc: 0.3804\n",
      "Epoch 16/30\n",
      "1467/1467 [==============================] - 2s 2ms/step - loss: 1.3229 - acc: 0.4744 - val_loss: 1.5321 - val_acc: 0.3865\n",
      "Epoch 17/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3235 - acc: 0.4778 - val_loss: 1.5384 - val_acc: 0.3865\n",
      "Epoch 18/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3059 - acc: 0.4819 - val_loss: 1.5410 - val_acc: 0.3926\n",
      "Epoch 19/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3169 - acc: 0.4744 - val_loss: 1.5405 - val_acc: 0.3558\n",
      "Epoch 20/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3063 - acc: 0.5010 - val_loss: 1.5482 - val_acc: 0.4049\n",
      "Epoch 21/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3108 - acc: 0.4819 - val_loss: 1.5505 - val_acc: 0.3865\n",
      "Epoch 22/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3206 - acc: 0.4710 - val_loss: 1.5758 - val_acc: 0.4110\n",
      "Epoch 23/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3012 - acc: 0.4833 - val_loss: 1.5459 - val_acc: 0.4049\n",
      "Epoch 24/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3000 - acc: 0.4881 - val_loss: 1.5665 - val_acc: 0.3620\n",
      "Epoch 25/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.2871 - acc: 0.5174 - val_loss: 1.5742 - val_acc: 0.3804\n",
      "Epoch 26/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.3016 - acc: 0.4860 - val_loss: 1.5835 - val_acc: 0.4049\n",
      "Epoch 27/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.2975 - acc: 0.4901 - val_loss: 1.5826 - val_acc: 0.3620\n",
      "Epoch 28/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.2962 - acc: 0.4874 - val_loss: 1.5800 - val_acc: 0.4110\n",
      "Epoch 29/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.2753 - acc: 0.5010 - val_loss: 1.5778 - val_acc: 0.3620\n",
      "Epoch 30/30\n",
      "1467/1467 [==============================] - 3s 2ms/step - loss: 1.2836 - acc: 0.4997 - val_loss: 1.5621 - val_acc: 0.3865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc1798eadd8>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, validation_data=(X_val, y_val), epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:54:39.871739Z",
     "start_time": "2018-11-29T17:54:39.071591Z"
    }
   },
   "outputs": [],
   "source": [
    "dev_7 = pd.read_csv(path_val_7, sep='\\t')\n",
    "\n",
    "seven_to_3 = {'-3: very negative emotional state can be inferred': 0,\n",
    "              '-2: moderately negative emotional state can be inferred': 0,\n",
    "              '-1: slightly negative emotional state can be inferred': 0,\n",
    "              '3: very positive emotional state can be inferred': 2,\n",
    "              '1: slightly positive emotional state can be inferred': 2,\n",
    "              '2: moderately positive emotional state can be inferred': 2,\n",
    "              '0: neutral or mixed emotional state can be inferred': 1}\n",
    "\n",
    "seven_to_7 = {'-3: very negative emotional state can be inferred': -3,\n",
    "              '-2: moderately negative emotional state can be inferred': -2,\n",
    "              '-1: slightly negative emotional state can be inferred': -1,\n",
    "              '3: very positive emotional state can be inferred': 3,\n",
    "              '1: slightly positive emotional state can be inferred': 1,\n",
    "              '2: moderately positive emotional state can be inferred': 2,\n",
    "              '0: neutral or mixed emotional state can be inferred': 0}\n",
    "\n",
    "\n",
    "dev_7['Sentiment'] = dev_7['Intensity Class'].apply(lambda x: seven_to_3[x])\n",
    "dev_7['Target'] = dev_7['Intensity Class'].apply(lambda x: seven_to_7[x])\n",
    "dev_7['Target_from_0'] = dev_7['Intensity Class'].apply(lambda x: seven_to_7[x] + 3)\n",
    "dev_7['Tweet'] = dev_7['Tweet'].apply(lambda x: standardization(x))\n",
    "\n",
    "dev_x, dev_y = dev_7['Tweet'], dev_7['Target_from_0']\n",
    "\n",
    "dev_sequences = tokenizer.texts_to_sequences(dev_x)\n",
    "dev_sequences = pad_sequences(dev_sequences, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:54:41.285000Z",
     "start_time": "2018-11-29T17:54:39.875552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson: 0.6448126409970354\n",
      "Quadratic kappa: 0.6407632098871721\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "y_pred = np.argmax(model.predict(dev_sequences, batch_size=128), axis=1)\n",
    "y_true = dev_y\n",
    "\n",
    "p = pearsonr(y_true, y_pred)[0]\n",
    "w = cohen_kappa_score(y_pred, y_true, weights='quadratic')\n",
    "\n",
    "print(f'Pearson: {p}')\n",
    "print(f'Quadratic kappa: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:57:16.463392Z",
     "start_time": "2018-11-29T17:57:16.438642Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_preprocess(filepath, test=False):\n",
    "    df = pd.read_csv(filepath, sep='\\t')\n",
    "\n",
    "    seven_to_7 = {'-3: very negative emotional state can be inferred': -3,\n",
    "                  '-2: moderately negative emotional state can be inferred': -2,\n",
    "                  '-1: slightly negative emotional state can be inferred': -1,\n",
    "                  '3: very positive emotional state can be inferred': 3,\n",
    "                  '1: slightly positive emotional state can be inferred': 1,\n",
    "                  '2: moderately positive emotional state can be inferred': 2,\n",
    "                  '0: neutral or mixed emotional state can be inferred': 0}\n",
    "\n",
    "    if test == False:\n",
    "        df['Target'] = df['Intensity Class'].apply(lambda x: seven_to_7[x])\n",
    "        df['Target_from_0'] = df['Intensity Class'].apply(lambda x: seven_to_7[x] + 3)\n",
    "\n",
    "    \n",
    "    df['Tweet_standardized'] = df['Tweet'].apply(lambda x: standardization(x))\n",
    "    \n",
    "    if not test:\n",
    "        tweets, targets = df['Tweet_standardized'], df['Target_from_0']\n",
    "    else:\n",
    "        tweets, targets = df['Tweet_standardized'], None\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(tweets)\n",
    "    sequences = pad_sequences(sequences, 32)\n",
    "    \n",
    "    return df, sequences, targets\n",
    "\n",
    "def predict(sequences, targets, test=False):\n",
    "    y_pred = np.argmax(model.predict(sequences, batch_size=128), axis=1)\n",
    "    y_true = targets\n",
    "    \n",
    "    if not test:\n",
    "        p = pearsonr(y_true, y_pred)[0]\n",
    "        print(f\"Pearson: {p}\")\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def output_to_csv(df, y_pred, output_filename='test_output.tsv'):\n",
    "    seven_to_7 = {-3: '-3: very negative emotional state can be inferred',\n",
    "              -2: '-2: moderately negative emotional state can be inferred',\n",
    "              -1: '-1: slightly negative emotional state can be inferred',\n",
    "              3: '3: very positive emotional state can be inferred',\n",
    "              1: '1: slightly positive emotional state can be inferred',\n",
    "              2: '2: moderately positive emotional state can be inferred',\n",
    "              0: '0: neutral or mixed emotional state can be inferred'}\n",
    "\n",
    "    y_out = [seven_to_7[y - 3] for y in y_pred]\n",
    "\n",
    "    output_file = df.assign(ic=pd.Series(y_out).values)\n",
    "    output_file = output_file[['ID', 'Tweet', 'Affect Dimension', 'ic']]\n",
    "    output_file = output_file.rename(index=str, columns={\"ic\": \"Intensity Class\"})\n",
    "\n",
    "    file = open(output_filename, 'w')\n",
    "    output_file.to_csv(path_or_buf=file, sep='\\t', index=False)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:57:23.671775Z",
     "start_time": "2018-11-29T17:57:22.058875Z"
    }
   },
   "outputs": [],
   "source": [
    "df, seq_test, y_test = read_preprocess('data/test.txt', test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T18:06:51.141250Z",
     "start_time": "2018-11-29T18:06:49.684713Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = predict(seq_test, y_test, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T18:06:51.196285Z",
     "start_time": "2018-11-29T18:06:51.147886Z"
    }
   },
   "outputs": [],
   "source": [
    "output_to_csv(df, y_pred, 'answer.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
