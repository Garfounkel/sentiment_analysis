{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word2Vec experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For this project we need to learn the embeddings of a Word2Vec algorithm.\n",
    "The embeddings are the weights of a single layer sequential neural network.\n",
    "On this notebook we will only focus on the methods used to create these embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Skip-gram\n",
    "\n",
    "Skip-Gram, as opposed to CBOW, is used to predict the context of a word with the word as input.\n",
    "\n",
    "TODO: explain method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot encoding\n",
    "\n",
    "TODO: explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used dataset\n",
    "\n",
    "TODO: find a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach : use Gensim to learn the embeddings\n",
    "\n",
    "TODO: gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to fit a hand-crafted Word2Vec model\n",
    "\n",
    "To better understand the underlying structure of the algorithm, we decided to implement our own neural network using Keras.\n",
    "\n",
    "We plan on building the vocabulary and the context from our dataset and use the context to train a Keras Neural Network that will be our Word2Vec model, and compare it with other Word2Vec models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preprocessing import NLTKTokenizer\n",
    "\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the context\n",
    "\n",
    "In this part, we will build the context that will be used for training our word2vec. The first step is to transform our texts as token. Once it is done, we have a list of words to process as a stream in our build_context function.\n",
    "This function will also build the vocabulary while processing the context of each word.\n",
    "\n",
    "Here, we only look at the word before and the word after our current word to define its context, the output represent each word from its index in the vocabulary.\n",
    "\n",
    "For example, with the sentence \"This is a test. a\", the vocabulary will look like \"{'This': 0, 'is': 1, 'a': 2, 'test': 3, '.': 4}\" and the context \"[(0, 1), (1, 0, 2), (2, 1, 3), (3, 2, 2), (2, 3, 4), (4, 2)]\" meaning that the word \"This\" have the index 1 as context, which is the word \"is\" in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = './data'\n",
    "\n",
    "train_3 = f'{PATH}/data_train_3.csv'\n",
    "test_3 = f'{PATH}/data_test_3.csv'\n",
    "train_7 = f'{PATH}/data_train_7.csv'\n",
    "train_16m_3 = f'{PATH}/training.1600000.processed.noemoticon.csv'\n",
    "\n",
    "tweets = pd.read_csv(train_3, sep='\\t', names=['ID', 'Class', 'Tweet'])\n",
    "sample = tweets.Tweet.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = 'a a This is a test. a'\n",
    "str2 = 'The quick brown fox jumped over the lazy dog.'\n",
    "str3 = 'Another text with a dog.'\n",
    "\n",
    "tweets = pd.Series(str1).append(pd.Series(str2)).append(pd.Series(str3))\n",
    "\n",
    "tokenizer = NLTKTokenizer(str2)\n",
    "\n",
    "dataset = pd.Series(iter(tokenizer))\n",
    "onehot = pd.get_dummies(dataset)\n",
    "\n",
    "\n",
    "def build_context(stream, queue = []):\n",
    "    for i in range(2):\n",
    "        token = next(stream)\n",
    "        if token not in build_context.vocab:\n",
    "            build_context.vocab[token] = build_context.count\n",
    "            build_context.count += 1\n",
    "        if len(queue) > 2:\n",
    "            queue.pop(0)\n",
    "            queue.pop(0)\n",
    "        queue.append(build_context.vocab[token])\n",
    "        \n",
    "    yield tuple(queue)\n",
    "\n",
    "    for token in stream:\n",
    "        if token not in build_context.vocab:\n",
    "            build_context.vocab[token] = build_context.count\n",
    "            build_context.count += 1\n",
    "        queue.append(build_context.vocab[token])\n",
    "        if len(queue) > 3:\n",
    "            queue.pop(0)\n",
    "        yield tuple([queue[i] for i in [1, 0, 2]])\n",
    "    queue.pop(0)\n",
    "    if (len(queue) < 2):\n",
    "        print(queue[0])\n",
    "    yield((queue[1], queue[0]))\n",
    "\n",
    "\n",
    "build_context.vocab = {}\n",
    "build_context.count = 0\n",
    "\n",
    "contexts = []\n",
    "for t in sample:\n",
    "    currTok = NLTKTokenizer(t)\n",
    "    contexts.append(list(build_context(iter(currTok))))\n",
    "    \n",
    "contexts = list(chain.from_iterable(contexts))\n",
    "print(sample)\n",
    "print('current context: ', contexts)\n",
    "print('vocabulary: ', build_context.vocab)\n",
    "print('voc size: ', build_context.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras NN model\n",
    "\n",
    "Arbitrary values for the moment, probably can still be optimized.\n",
    "\n",
    "TODO: desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=build_context.count))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.add(Dense(build_context.count))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_onehot(index, n):\n",
    "    res = np.zeros((1, n))\n",
    "    res[0, index] = 1\n",
    "    return res\n",
    "\n",
    "\n",
    "def context_to_onehot(neighbors, n):\n",
    "    res = np.zeros((1, n))\n",
    "    for index in neighbors:\n",
    "        res[0, index] = 1\n",
    "    return res / len(set(neighbors))\n",
    "\n",
    "\n",
    "X_train = np.array([index_to_onehot(x[0], build_context.count)[0] for x in contexts])\n",
    "Y_train = np.array([context_to_onehot(x[1:], build_context.count)[0] for x in contexts])\n",
    "\n",
    "\n",
    "print(Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 15\n",
    "batch_size = 32\n",
    "\n",
    "model.summary()\n",
    "model.fit(X_train, Y_train, epochs=epoch, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_of_word(word, nb):\n",
    "    '''\n",
    "    Looks for the K closests element in the context of a word and prints the result\n",
    "    '''\n",
    "    if word in build_context.vocab:\n",
    "        index = build_context.vocab[word]\n",
    "    else:\n",
    "        print('No such word in vocabulary')\n",
    "        return\n",
    "    \n",
    "    x_test = index_to_onehot(index, build_context.count)\n",
    "    y_test = model.predict(x_test, verbose=1)\n",
    "    context_index = np.argmax(y_test)\n",
    "    closest_words = np.argpartition(y_test[0], -nb)[-nb:]\n",
    "    closest_words.sort()\n",
    "    \n",
    "    i = 0\n",
    "    for k, v in build_context.vocab.items():\n",
    "        if v == closest_words[i]:\n",
    "            i += 1\n",
    "            print('closest word in context is: ', k)\n",
    "            if i == nb:\n",
    "                break\n",
    "\n",
    "context_of_word('how', 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
