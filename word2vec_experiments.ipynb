{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word2Vec experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For this project we need to learn the embeddings of a Word2Vec algorithm.\n",
    "The embeddings are the weights of a single layer sequential neural network.\n",
    "On this notebook we will only focus on the methods used to create these embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Skip-gram\n",
    "\n",
    "Skip-Gram, as opposed to CBOW, is used to predict the context of a word with the word as input.\n",
    "\n",
    "TODO: explain method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot encoding\n",
    "\n",
    "TODO: explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used dataset\n",
    "\n",
    "TODO: find a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach : use Gensim to learn the embeddings\n",
    "\n",
    "TODO: gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to fit a hand-crafted Word2Vec model\n",
    "\n",
    "To better understand the underlying structure of the algorithm, we decided to implement our own neural network using Keras.\n",
    "\n",
    "We plan on building the vocabulary and the context from our dataset and use the context to train a Keras Neural Network that will be our Word2Vec model, and compare it with other Word2Vec models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preprocessing import NLTKTokenizer\n",
    "\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load decompress_dataset.py\n",
    "import json\n",
    "import subprocess\n",
    "import glob\n",
    "#import mysql.connector\n",
    "#from tweet_scraper import insert_db\n",
    "\n",
    "\n",
    "def tweet_generator():\n",
    "    tar_files = glob.glob('./*.tar')\n",
    "\n",
    "    for tar_file in tar_files:\n",
    "        subprocess.call(['tar', '-xf', tar_file])\n",
    "        bz2_files = glob.glob('./*/*/*/*/*.json.bz2')\n",
    "        for bz2_file in bz2_files:\n",
    "            subprocess.call(['bzip2', '-d', bz2_file])\n",
    "            file = bz2_file.split('.')\n",
    "            file = '.'.join(file[:-1])\n",
    "            with open(file, \"r\") as ins:\n",
    "                for line in ins:\n",
    "                    yield json.loads(line)\n",
    "            subprocess.call([\"rm\", file])\n",
    "\n",
    "\n",
    "def get_tweet_text(tweet):\n",
    "    if \"extended_tweet\" in tweet:\n",
    "        return tweet[\"extended_tweet\"][\"full_text\"]\n",
    "    return tweet[\"text\"]\n",
    "\n",
    "\n",
    "class Tweet:\n",
    "    def __init__(self, id, text):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "\n",
    "def decompress():\n",
    "    '''\n",
    "    mySQLdb = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"nicolas\",\n",
    "        passwd=\"nicolas\",\n",
    "        database=\"tweets\",\n",
    "    )\n",
    "'''\n",
    "    tweets = filter(lambda x: \"lang\" in x, tweet_generator())\n",
    "    tweets = filter(lambda x: x[\"lang\"] == \"en\", tweets)\n",
    "    tweets = filter(lambda x: \"retweeted_status\" not in x, tweets)\n",
    "    tweets = map(lambda x: Tweet(x[\"id\"], get_tweet_text(x)), tweets)\n",
    "    tweets = filter(lambda x: '…' not in x.text, tweets)\n",
    "    return tweets\n",
    "    #insert_db(mySQLdb, tweets)\n",
    "\n",
    "tweets = decompress()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = []\n",
    "for _ in range(100):\n",
    "    try:\n",
    "        val.append(NLTKTokenizer(next(tweets).text))\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-6078d02d80f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcurrTok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNLTKTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurrTok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtweet_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Sentiment_analysis/sentiment_analysis/text_preprocessing.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Sentiment_analysis/sentiment_analysis/text_preprocessing.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNLTKTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \"\"\"\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    132\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \"\"\"\n\u001b[1;32m     96\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \"\"\"\n\u001b[0;32m-> 1235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \"\"\"\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \"\"\"\n\u001b[1;32m   1313\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1288\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "tweet_list = []\n",
    "\n",
    "for tweet in val:\n",
    "    currTok = NLTKTokenizer(tweet)\n",
    "    sentence = []\n",
    "    for token in currTok:\n",
    "        sentence.append(token)\n",
    "    tweet_list.append(sentence)\n",
    "\n",
    "model = Word2Vec(tweet_list, size=300, sg=1, window=1, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the context\n",
    "\n",
    "In this part, we will build the context that will be used for training our word2vec. The first step is to transform our texts as token. Once it is done, we have a list of words to process as a stream in our build_context function.\n",
    "This function will also build the vocabulary while processing the context of each word.\n",
    "\n",
    "Here, we only look at the word before and the word after our current word to define its context, the output represent each word from its index in the vocabulary.\n",
    "\n",
    "For example, with the sentence \"This is a test. a\", the vocabulary will look like \"{'This': 0, 'is': 1, 'a': 2, 'test': 3, '.': 4}\" and the context \"[(0, 1), (1, 0, 2), (2, 1, 3), (3, 2, 2), (2, 3, 4), (4, 2)]\" meaning that the word \"This\" have the index 1 as context, which is the word \"is\" in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created_at\n",
      "id\n",
      "id_str\n",
      "text\n",
      "source\n",
      "truncated\n",
      "in_reply_to_status_id\n",
      "in_reply_to_status_id_str\n",
      "in_reply_to_user_id\n",
      "in_reply_to_user_id_str\n",
      "in_reply_to_screen_name\n",
      "user\n",
      "geo\n",
      "coordinates\n",
      "place\n",
      "contributors\n",
      "is_quote_status\n",
      "quote_count\n",
      "reply_count\n",
      "retweet_count\n",
      "favorite_count\n",
      "entities\n",
      "favorited\n",
      "retweeted\n",
      "filter_level\n",
      "lang\n",
      "timestamp_ms\n",
      "捜しやすいリハ着ありがとー(๑´ლ`๑)ﾌ°ﾌ°♡\n"
     ]
    }
   ],
   "source": [
    "PATH = './data'\n",
    "\n",
    "train_3 = f'{PATH}/data_train_3.csv'\n",
    "test_3 = f'{PATH}/data_test_3.csv'\n",
    "train_7 = f'{PATH}/data_train_7.csv'\n",
    "train_16m_3 = f'{PATH}/training.1600000.processed.noemoticon.csv'\n",
    "\n",
    "tweets = pd.read_csv(train_3, sep='\\t', names=['ID', 'Class', 'Tweet'])\n",
    "sample = tweets.Tweet.head(500)\n",
    "\n",
    "tweets_dir = f'{PATH}/2017'\n",
    "tweets = []\n",
    "for line in  open('./data/2017/00.json'):\n",
    "    tweets.append(json.loads(line))\n",
    "\n",
    "for key in tweets[0]:\n",
    "    print(key)\n",
    "\n",
    "print(tweets[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('eyes\\\\u002c', 0.00026797055), ('tube', 0.00026796898), ('Reason', 0.00026796872), ('biscuit', 0.00026796845), ('Anybody', 0.000267968), ('delete', 0.00026796773), ('summary', 0.00026796758), ('drinking', 0.00026796758), ('MMFlint', 0.00026796694), ('Ethan_Hammer', 0.0002679666)]\n"
     ]
    }
   ],
   "source": [
    "tweet_list = []\n",
    "\n",
    "for tweet in sample:\n",
    "    currTok = NLTKTokenizer(tweet)\n",
    "    sentence = []\n",
    "    for token in currTok:\n",
    "        sentence.append(token)\n",
    "    tweet_list.append(sentence)\n",
    "\n",
    "model = Word2Vec(tweet_list, size=300, sg=1, window=1, min_count=1)\n",
    "print(model.predict_output_word(['How', 'are', 'you']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jumped\n",
      "over\n",
      "the\n",
      "lazy\n",
      "dog\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "str1 = 'a a This is a test. a'\n",
    "str2 = 'The quick brown fox jumped over the lazy dog.'\n",
    "str3 = 'Another text with a dog.'\n",
    "\n",
    "tweets = pd.Series(str1).append(pd.Series(str2)).append(pd.Series(str3))\n",
    "\n",
    "tokenizer = NLTKTokenizer(str2)\n",
    "\n",
    "dataset = pd.Series(iter(tokenizer))\n",
    "onehot = pd.get_dummies(dataset)\n",
    "\n",
    "\n",
    "def build_context(stream, queue = []):\n",
    "    for i in range(2):\n",
    "        token = next(stream)\n",
    "        if token not in build_context.vocab:\n",
    "            build_context.vocab[token] = build_context.count\n",
    "            build_context.count += 1\n",
    "        if len(queue) > 2:\n",
    "            queue.pop(0)\n",
    "            queue.pop(0)\n",
    "        queue.append(build_context.vocab[token])\n",
    "        \n",
    "    yield tuple(queue)\n",
    "\n",
    "    for token in stream:\n",
    "        if token not in build_context.vocab:\n",
    "            build_context.vocab[token] = build_context.count\n",
    "            build_context.count += 1\n",
    "        queue.append(build_context.vocab[token])\n",
    "        if len(queue) > 3:\n",
    "            queue.pop(0)\n",
    "        yield tuple([queue[i] for i in [1, 0, 2]])\n",
    "    queue.pop(0)\n",
    "    if (len(queue) < 2):\n",
    "        print(queue[0])\n",
    "    yield((queue[1], queue[0]))\n",
    "\n",
    "\n",
    "build_context.vocab = {}\n",
    "build_context.count = 0\n",
    "\n",
    "contexts = []\n",
    "for t in sample:\n",
    "    currTok = NLTKTokenizer(t)\n",
    "    contexts.append(list(build_context(iter(currTok))))\n",
    "    \n",
    "contexts = list(chain.from_iterable(contexts))\n",
    "\n",
    "print(sample)\n",
    "print('current context: ', contexts)\n",
    "print('vocabulary: ', build_context.vocab)\n",
    "print('voc size: ', build_context.count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(size=300, sg=1, window=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras NN model\n",
    "\n",
    "Arbitrary values for the moment, probably can still be optimized.\n",
    "\n",
    "TODO: desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=build_context.count))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.add(Dense(build_context.count))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  1.  0.  ... 0.  0.  0. ]\n",
      " [0.5 0.  0.5 ... 0.  0.  0. ]\n",
      " [0.  0.5 0.  ... 0.  0.  0. ]\n",
      " ...\n",
      " [0.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.  0.  ... 0.  0.  0.5]\n",
      " [0.  0.  0.  ... 0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "def index_to_onehot(index, n):\n",
    "    res = np.zeros((1, n))\n",
    "    res[0, index] = 1\n",
    "    return res\n",
    "\n",
    "\n",
    "def context_to_onehot(neighbors, n):\n",
    "    res = np.zeros((1, n))\n",
    "    for index in neighbors:\n",
    "        res[0, index] = 1\n",
    "    return res / len(set(neighbors))\n",
    "\n",
    "\n",
    "X_train = np.array([index_to_onehot(x[0], build_context.count)[0] for x in contexts])\n",
    "Y_train = np.array([context_to_onehot(x[1:], build_context.count)[0] for x in contexts])\n",
    "\n",
    "\n",
    "print(Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 300)               1119900   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3732)              1123332   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3732)              0         \n",
      "=================================================================\n",
      "Total params: 2,243,232\n",
      "Trainable params: 2,243,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "11309/11309 [==============================] - 14s 1ms/step - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 2/15\n",
      "11309/11309 [==============================] - 12s 1ms/step - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 3/15\n",
      "11309/11309 [==============================] - 12s 1ms/step - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 4/15\n",
      "11309/11309 [==============================] - 15s 1ms/step - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 5/15\n",
      "11309/11309 [==============================] - 14s 1ms/step - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 6/15\n",
      "11309/11309 [==============================] - 14s 1ms/step - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 7/15\n",
      "11309/11309 [==============================] - 13s 1ms/step - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 8/15\n",
      "11309/11309 [==============================] - 13s 1ms/step - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 9/15\n",
      "11309/11309 [==============================] - 13s 1ms/step - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 10/15\n",
      "11309/11309 [==============================] - 13s 1ms/step - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 11/15\n",
      "11309/11309 [==============================] - 13s 1ms/step - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 12/15\n",
      "11309/11309 [==============================] - 13s 1ms/step - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 13/15\n",
      "11309/11309 [==============================] - 13s 1ms/step - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 14/15\n",
      "11309/11309 [==============================] - 13s 1ms/step - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 15/15\n",
      "11309/11309 [==============================] - 13s 1ms/step - loss: 0.0019 - acc: 0.9995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b3aea4d68>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = 15\n",
    "batch_size = 32\n",
    "\n",
    "model.summary()\n",
    "model.fit(X_train, Y_train, epochs=epoch, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "closest word in context is:  a\n",
      "closest word in context is:  and\n",
      "closest word in context is:  you\n",
      "closest word in context is:  I\n"
     ]
    }
   ],
   "source": [
    "def context_of_word(word, nb):\n",
    "    '''\n",
    "    Looks for the K closests element in the context of a word and prints the result\n",
    "    '''\n",
    "    if word in build_context.vocab:\n",
    "        index = build_context.vocab[word]\n",
    "    else:\n",
    "        print('No such word in vocabulary')\n",
    "        return\n",
    "    \n",
    "    x_test = index_to_onehot(index, build_context.count)\n",
    "    y_test = model.predict(x_test, verbose=1)\n",
    "    context_index = np.argmax(y_test)\n",
    "    closest_words = np.argpartition(y_test[0], -nb)[-nb:]\n",
    "    closest_words.sort()\n",
    "    \n",
    "    i = 0\n",
    "    for k, v in build_context.vocab.items():\n",
    "        if v == closest_words[i]:\n",
    "            i += 1\n",
    "            print('closest word in context is: ', k)\n",
    "            if i == nb:\n",
    "                break\n",
    "\n",
    "context_of_word('how', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
