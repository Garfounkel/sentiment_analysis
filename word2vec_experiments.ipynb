{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word2Vec experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For this project we need to learn the embeddings of a Word2Vec algorithm.\n",
    "The embeddings are the weights of a single layer sequential neural network.\n",
    "On this notebook we will only focus on the methods used to create these embeddings\n",
    "\n",
    "## Skip-gram\n",
    "\n",
    "TODO: explain method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot encoding\n",
    "\n",
    "TODO: explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used dataset\n",
    "\n",
    "TODO: find a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach : use Gensim to learn the embeddings\n",
    "\n",
    "TODO: gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to fit a hand-crafted Word2Vec model\n",
    "\n",
    "To better understand the underlying structure of the algorithm, we decided to implement our own neural network using Keras.\n",
    "\n",
    "TODO: fill description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from text_preprocessing import NLTKTokenizer\n",
    "\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the context\n",
    "\n",
    "TODO: fill description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 0, 2), (2, 1, 3), (3, 2, 2), (2, 3, 4), (4, 2)]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = NLTKTokenizer('This is a test a .')\n",
    "\n",
    "\n",
    "dataset = pd.Series(iter(tokenizer))\n",
    "\n",
    "\n",
    "onehot = pd.get_dummies(dataset)\n",
    "\n",
    "\n",
    "def test(stream):\n",
    "    queue = [0, 1]\n",
    "    test.count = 2\n",
    "    test.vocab[next(stream)] = 0\n",
    "    test.vocab[next(stream)] = 1\n",
    "    yield tuple(queue)\n",
    "    for token in stream:\n",
    "        if token not in test.vocab:\n",
    "            test.vocab[token] = test.count\n",
    "            test.count += 1\n",
    "        queue.append(test.vocab[token])\n",
    "        if len(queue) > 3:\n",
    "            queue.pop(0)\n",
    "        yield tuple([queue[i] for i in [1, 0, 2]])\n",
    "    queue.pop(0)\n",
    "    yield((queue[1], queue[0]))\n",
    "\n",
    "\n",
    "test.vocab = {}\n",
    "test.count = 0\n",
    "\n",
    "\n",
    "contexts = list(test(iter(tokenizer)))\n",
    "print(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras NN model\n",
    "\n",
    "TODO: desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=test.count))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.add(Dense(test.count))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  1.  0.  0.  0. ]\n",
      " [0.5 0.  0.5 0.  0. ]\n",
      " [0.  0.5 0.  0.5 0. ]\n",
      " [0.  0.  1.  0.  0. ]\n",
      " [0.  0.  0.  0.5 0.5]\n",
      " [0.  0.  1.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def index_to_onehot(index, n):\n",
    "    res = np.zeros((1, n))\n",
    "    res[0, index] = 1\n",
    "    return res\n",
    "\n",
    "\n",
    "def context_to_onehot(neighbors, n):\n",
    "    res = np.zeros((1, n))\n",
    "    for index in neighbors:\n",
    "        res[0, index] = 1\n",
    "    return res / len(set(neighbors))\n",
    "\n",
    "\n",
    "X_train = np.array([index_to_onehot(x[0], test.count)[0] for x in contexts])\n",
    "Y_train = np.array([context_to_onehot(x[1:], test.count)[0] for x in contexts])\n",
    "\n",
    "\n",
    "print(Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 300)               1800      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1505      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 3,305\n",
      "Trainable params: 3,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.5050 - acc: 0.7000\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 0s 275us/step - loss: 0.4792 - acc: 0.7000\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 0s 255us/step - loss: 0.4611 - acc: 0.7000\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 0s 301us/step - loss: 0.4462 - acc: 0.7000\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 0s 265us/step - loss: 0.4331 - acc: 0.7000\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 0s 255us/step - loss: 0.4213 - acc: 0.7000\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 0s 259us/step - loss: 0.4103 - acc: 0.7000\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 0s 307us/step - loss: 0.4000 - acc: 0.7000\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 0s 715us/step - loss: 0.3903 - acc: 0.7000\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 0s 310us/step - loss: 0.3811 - acc: 0.7000\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 0s 339us/step - loss: 0.3723 - acc: 0.7000\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 0s 302us/step - loss: 0.3639 - acc: 0.7000\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 0s 261us/step - loss: 0.3558 - acc: 0.7333\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 0s 509us/step - loss: 0.3480 - acc: 0.7667\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 0s 451us/step - loss: 0.3406 - acc: 0.7667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdaa0949d30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = 15\n",
    "batch_size = 32\n",
    "\n",
    "model.summary()\n",
    "model.fit(X_train, Y_train, epochs=epoch, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
