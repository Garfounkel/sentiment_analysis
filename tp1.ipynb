{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T13:30:02.777559Z",
     "start_time": "2018-10-24T13:29:59.996642Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras import backend as K\n",
    "\n",
    "# num_cores = 4\n",
    "# CPU = True\n",
    "# GPU = False\n",
    "\n",
    "# if GPU:\n",
    "#     num_GPU = 1\n",
    "#     num_CPU = 1\n",
    "# if CPU:\n",
    "#     num_CPU = 1\n",
    "#     num_GPU = 0\n",
    "\n",
    "# config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "#         inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "#         device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "# session = tf.Session(config=config)\n",
    "# K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T11:18:46.930735Z",
     "start_time": "2018-11-01T11:18:43.508832Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import *\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizer import tokenizer as tweet_tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import *\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import * \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "PATH = '/home/epita/sim/sentiment_analysis/data/'\n",
    "\n",
    "train_3 = f'{PATH}/data_train_3.csv'\n",
    "test_3 = f'{PATH}/data_test_3.csv'\n",
    "train_7 = f'{PATH}/data_train_7.csv'\n",
    "train_16m_3 = f'{PATH}/training.1600000.processed.noemoticon.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T19:13:44.756774Z",
     "start_time": "2018-10-20T19:13:44.742287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[6, 2, 7, 1, 4, 4, 6, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "\n",
    "result = one_hot(text, round(vocab_size*1.3))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Tokenizer sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:46:21.628712Z",
     "start_time": "2018-10-24T12:46:21.609088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = tweet_tokenizer.TweetTokenizer(preserve_case=False, preserve_url=False)\n",
    "\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "\n",
    "T.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer / Stemmer samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T19:18:52.094815Z",
     "start_time": "2018-10-20T19:18:52.072891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem studying: studi\n",
      "Lemmatise studying: studying\n",
      "Lemmatise studying: study\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    " \n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    " \n",
    "print(\"Stem %s: %s\" % (\"studying\", stemmer.stem(\"studying\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"studying\", lemmatiser.lemmatize(\"studying\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"studying\", lemmatiser.lemmatize(\"studying\", pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T19:26:23.149301Z",
     "start_time": "2018-10-20T19:26:23.130556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('simple', 'JJ'), ('sentence', 'NN'), ('that', 'WDT'), ('would', 'MD'), ('be', 'VB'), ('allowing', 'VBG'), ('us', 'PRP'), ('to', 'TO'), ('try', 'VB'), ('lemmatizing', 'VBG')]\n",
      "['This', 'be', 'a', 'simple', 'sentence', 'that', 'would', 'be', 'allow', 'u', 'to', 'try', 'lemmatizing']\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "tag_map = defaultdict(lambda: wn.NOUN, {'J': wn.ADJ, 'V': wn.VERB, 'R': wn.ADV})\n",
    "\n",
    "s = \"This is a simple sentence that would be allowing us to try lemmatizing\"\n",
    "\n",
    "tokens = word_tokenize(s)\n",
    "tokens_pos = pos_tag(tokens)\n",
    " \n",
    "print(tokens_pos)\n",
    "\n",
    "\n",
    "lems = [lemmatiser.lemmatize(word, tag_map[pos[0]]) for word, pos in tokens_pos]\n",
    "print(lems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serious business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Train_16m_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T11:34:00.813555Z",
     "start_time": "2018-11-01T11:33:55.355808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1136298</th>\n",
       "      <td>4</td>\n",
       "      <td>1976586207</td>\n",
       "      <td>@Doc_Sprocket sorry - i thought i wasn't going...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82391</th>\n",
       "      <td>0</td>\n",
       "      <td>1753038955</td>\n",
       "      <td>horrible headache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883382</th>\n",
       "      <td>4</td>\n",
       "      <td>1686122324</td>\n",
       "      <td>YES! ... finished my assignment! 1 down and 3 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473925</th>\n",
       "      <td>4</td>\n",
       "      <td>2065642051</td>\n",
       "      <td>best #kiss i ever seen  http://bit.ly/kdepj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795603</th>\n",
       "      <td>0</td>\n",
       "      <td>2327426618</td>\n",
       "      <td>thinking she failed her geo exam, and possibly...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target          id                                               text\n",
       "1136298       4  1976586207  @Doc_Sprocket sorry - i thought i wasn't going...\n",
       "82391         0  1753038955                                 horrible headache \n",
       "883382        4  1686122324  YES! ... finished my assignment! 1 down and 3 ...\n",
       "1473925       4  2065642051        best #kiss i ever seen  http://bit.ly/kdepj\n",
       "795603        0  2327426618  thinking she failed her geo exam, and possibly..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_16m = pd.read_csv(train_16m_3, sep=',', encoding='latin-1').drop(columns=['date', 'flag', 'user'])\n",
    "\n",
    "print(tweets_16m.shape)\n",
    "tweets_16m.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T12:06:33.553573Z",
     "start_time": "2018-11-01T11:37:19.546472Z"
    }
   },
   "outputs": [],
   "source": [
    "from TP_transfer_learning_2018 import *\n",
    "from TP_transfer_learning_2018.preprocessing import standardization\n",
    "\n",
    "\n",
    "tweets_16m['target'] = tweets_16m['target'].apply(lambda x: {0: 0, 2: 1, 4: 2}[x])  # Neg (0), Neu (2), Pos (4)\n",
    "tweets_16m['text'] = tweets_16m['text'].apply(lambda x: standardization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T12:06:33.764607Z",
     "start_time": "2018-11-01T12:06:33.577280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>174553</th>\n",
       "      <td>0</td>\n",
       "      <td>1964566165</td>\n",
       "      <td>babe u aint read tweet phone fall apart new co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235416</th>\n",
       "      <td>0</td>\n",
       "      <td>1979777211</td>\n",
       "      <td>not cold go get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901553</th>\n",
       "      <td>2</td>\n",
       "      <td>1694256723</td>\n",
       "      <td>get new frens-frm germany mexico n france havi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913630</th>\n",
       "      <td>2</td>\n",
       "      <td>1752671998</td>\n",
       "      <td>im not cookie wont comment heh girl sing crush...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832154</th>\n",
       "      <td>2</td>\n",
       "      <td>1557553994</td>\n",
       "      <td>ah sunday morning nice day get realize money a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target          id                                               text\n",
       "174553       0  1964566165  babe u aint read tweet phone fall apart new co...\n",
       "235416       0  1979777211                                    not cold go get\n",
       "901553       2  1694256723  get new frens-frm germany mexico n france havi...\n",
       "913630       2  1752671998  im not cookie wont comment heh girl sing crush...\n",
       "832154       2  1557553994  ah sunday morning nice day get realize money a..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'night twitterverse seek ever elusive prince sleep sweet dream u good morning ever apply zzzz'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(tweets_16m.sample(5))\n",
    "tweets_16m.sample(5)['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T12:06:34.472709Z",
     "start_time": "2018-11-01T12:06:34.039450Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_16m.to_feather('pickles/train_16m_3.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Train_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T11:19:02.082629Z",
     "start_time": "2018-11-01T11:19:01.850229Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50333, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(train_3, sep='\\t', names=['ID', 'Class', 'Tweet'])\n",
    "\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T19:03:22.353439Z",
     "start_time": "2018-10-31T19:03:22.309694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13666</th>\n",
       "      <td>522735017957814272</td>\n",
       "      <td>neutral</td>\n",
       "      <td>View the program for the new RNA Nanotechnolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9474</th>\n",
       "      <td>251159660458024960</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@SouthSideAsylum no back to watching Jim Henso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28417</th>\n",
       "      <td>635916750328991746</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Kris Bryant homered in the bottom of the 9th. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31083</th>\n",
       "      <td>638338687902744576</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Canadians would actually love it if Scott Walk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47663</th>\n",
       "      <td>260158752907468800</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Did you know that every Monday is Facebook Mon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID    Class  \\\n",
       "13666  522735017957814272  neutral   \n",
       "9474   251159660458024960  neutral   \n",
       "28417  635916750328991746  neutral   \n",
       "31083  638338687902744576  neutral   \n",
       "47663  260158752907468800  neutral   \n",
       "\n",
       "                                                   Tweet  \n",
       "13666  View the program for the new RNA Nanotechnolog...  \n",
       "9474   @SouthSideAsylum no back to watching Jim Henso...  \n",
       "28417  Kris Bryant homered in the bottom of the 9th. ...  \n",
       "31083  Canadians would actually love it if Scott Walk...  \n",
       "47663  Did you know that every Monday is Facebook Mon...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T11:20:34.446157Z",
     "start_time": "2018-11-01T11:19:15.874251Z"
    }
   },
   "outputs": [],
   "source": [
    "from TP_transfer_learning_2018 import *\n",
    "from TP_transfer_learning_2018.preprocessing import standardization\n",
    "\n",
    "\n",
    "tweets['Sentiment'] = tweets['Class'].apply(lambda x: {'negative': 0, 'neutral': 1, 'positive': 2}[x])\n",
    "tweets['Tweet'] = tweets['Tweet'].apply(lambda x: standardization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T11:20:34.490519Z",
     "start_time": "2018-11-01T11:20:34.454390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15079</th>\n",
       "      <td>522833226662813696</td>\n",
       "      <td>positive</td>\n",
       "      <td>ben affleck henry cavill city suck know 3rd se...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43712</th>\n",
       "      <td>628587233365266432</td>\n",
       "      <td>positive</td>\n",
       "      <td>galaxy note event schedule august 13 seem thou...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10248</th>\n",
       "      <td>264075183738998785</td>\n",
       "      <td>positive</td>\n",
       "      <td>.. morning austin ... go bed .. hahha 8: 45pm ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40095</th>\n",
       "      <td>680397873910005760</td>\n",
       "      <td>neutral</td>\n",
       "      <td>let christ king israel come cross may see believe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47884</th>\n",
       "      <td>247139201496260608</td>\n",
       "      <td>neutral</td>\n",
       "      <td>jcc demand rop workcharge employee urge chief ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID     Class  \\\n",
       "15079  522833226662813696  positive   \n",
       "43712  628587233365266432  positive   \n",
       "10248  264075183738998785  positive   \n",
       "40095  680397873910005760   neutral   \n",
       "47884  247139201496260608   neutral   \n",
       "\n",
       "                                                   Tweet  Sentiment  \n",
       "15079  ben affleck henry cavill city suck know 3rd se...          2  \n",
       "43712  galaxy note event schedule august 13 seem thou...          2  \n",
       "10248  .. morning austin ... go bed .. hahha 8: 45pm ...          2  \n",
       "40095  let christ king israel come cross may see believe          1  \n",
       "47884  jcc demand rop workcharge employee urge chief ...          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'lumia xl rumor t-mobile may get first dibs official press render cityman ...'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(tweets.sample(5))\n",
    "tweets.sample(5)['Tweet'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T12:06:34.034157Z",
     "start_time": "2018-11-01T12:06:33.769108Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets.to_feather('pickles/train_3.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T11:20:35.305863Z",
     "start_time": "2018-11-01T11:20:34.493448Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_tweets, sentiments = tweets['Tweet'], tweets['Sentiment']\n",
    "train_tweets, sentiments = tweets_16m['text'], tweets_16m['target']\n",
    "\n",
    "all_tweets = train_tweets # + test_tweets\n",
    "tokenizer = Tokenizer(filters=' ')\n",
    "tokenizer.fit_on_texts(all_tweets)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T11:20:43.640152Z",
     "start_time": "2018-11-01T11:20:42.538711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_tweets)\n",
    "\n",
    "sequences = train_sequences # + test_sequences\n",
    "MAX_SEQUENCE_LENGTH = 0\n",
    "for elt in sequences:\n",
    "    if len(elt) > MAX_SEQUENCE_LENGTH:\n",
    "        MAX_SEQUENCE_LENGTH = len(elt)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T11:20:48.239233Z",
     "start_time": "2018-11-01T11:20:47.801129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50333, 32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences = pad_sequences(train_sequences, MAX_SEQUENCE_LENGTH)\n",
    "train_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T11:22:18.848377Z",
     "start_time": "2018-11-01T11:20:56.320415Z"
    }
   },
   "outputs": [],
   "source": [
    "googlenews_w2v = KeyedVectors.load_word2vec_format('data/embeddings/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T11:22:24.680175Z",
     "start_time": "2018-11-01T11:22:23.765093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 300)\n",
      "(36968, 300)\n"
     ]
    }
   ],
   "source": [
    "targets = to_categorical(sentiments, 3)\n",
    "nb_words = len(word_index) + 1\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "oov = []  # Out of vocabulary\n",
    "oov.append((np.random.rand(EMBEDDING_DIM) * 2.0) - 1.0)\n",
    "oov = oov / np.linalg.norm(oov)\n",
    "\n",
    "print(oov.shape)\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in googlenews_w2v.vocab:\n",
    "        embedding_matrix[i] = googlenews_w2v.word_vec(word)\n",
    "    else:\n",
    "        embedding_matrix[i] = oov\n",
    "        \n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T11:22:44.043182Z",
     "start_time": "2018-11-01T11:22:41.134561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Claim memory back from this very large object we don't use anymore\n",
    "del googlenews_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T11:22:56.560508Z",
     "start_time": "2018-11-01T11:22:56.491459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 35233 samples\n",
      "validation set: 15100 samples\n",
      "x_train: (35233, 32)\n",
      "y_train: (35233, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_sequences, targets, test_size=0.3)\n",
    "\n",
    "print('training set: ' + str(len(X_train)) + ' samples')\n",
    "print('validation set: ' + str(len(X_val)) + ' samples')\n",
    "\n",
    "print('x_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T11:23:08.773513Z",
     "start_time": "2018-11-01T11:23:03.235243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 32, 300)           11090400  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 32, 300)           541200    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 32, 300)           541200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 300)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 28803     \n",
      "=================================================================\n",
      "Total params: 12,201,603\n",
      "Trainable params: 1,111,203\n",
      "Non-trainable params: 11,090,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                    input_length=MAX_SEQUENCE_LENGTH, trainable=False, name='embedding_layer'))\n",
    "model.add(Dropout(0.3))\n",
    "# model.add(LSTM(150))\n",
    "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-2), metrics=['acc'])\n",
    "          \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T11:27:00.328634Z",
     "start_time": "2018-11-01T11:24:40.515815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35233 samples, validate on 15100 samples\n",
      "Epoch 1/5\n",
      "35200/35233 [============================>.] - ETA: 0s - loss: 0.8946 - acc: 0.5915"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a51d250dfbf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, validation_data=(X_val, y_val), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T22:04:07.622638Z",
     "start_time": "2018-10-31T21:31:33.830548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35233 samples, validate on 15100 samples\n",
      "Epoch 1/15\n",
      "35233/35233 [==============================] - 131s 4ms/step - loss: 0.6115 - acc: 0.7292 - val_loss: 0.7785 - val_acc: 0.6544\n",
      "Epoch 2/15\n",
      "35233/35233 [==============================] - 131s 4ms/step - loss: 0.5821 - acc: 0.7434 - val_loss: 0.7843 - val_acc: 0.6644\n",
      "Epoch 3/15\n",
      "35233/35233 [==============================] - 129s 4ms/step - loss: 0.5598 - acc: 0.7580 - val_loss: 0.8256 - val_acc: 0.6501\n",
      "Epoch 4/15\n",
      "35233/35233 [==============================] - 131s 4ms/step - loss: 0.5369 - acc: 0.7709 - val_loss: 0.8124 - val_acc: 0.6609\n",
      "Epoch 5/15\n",
      "35233/35233 [==============================] - 132s 4ms/step - loss: 0.5178 - acc: 0.7779 - val_loss: 0.8663 - val_acc: 0.6523\n",
      "Epoch 6/15\n",
      "35233/35233 [==============================] - 132s 4ms/step - loss: 0.5016 - acc: 0.7857 - val_loss: 0.8337 - val_acc: 0.6562\n",
      "Epoch 7/15\n",
      "35233/35233 [==============================] - 130s 4ms/step - loss: 0.4966 - acc: 0.7890 - val_loss: 0.9111 - val_acc: 0.6448\n",
      "Epoch 8/15\n",
      "35233/35233 [==============================] - 132s 4ms/step - loss: 0.4776 - acc: 0.7993 - val_loss: 0.9321 - val_acc: 0.6476\n",
      "Epoch 9/15\n",
      "35233/35233 [==============================] - 125s 4ms/step - loss: 0.4681 - acc: 0.8022 - val_loss: 0.9310 - val_acc: 0.6535\n",
      "Epoch 10/15\n",
      "35233/35233 [==============================] - 124s 4ms/step - loss: 0.4557 - acc: 0.8081 - val_loss: 0.9320 - val_acc: 0.6569\n",
      "Epoch 11/15\n",
      "35233/35233 [==============================] - 132s 4ms/step - loss: 0.4478 - acc: 0.8115 - val_loss: 0.9320 - val_acc: 0.6587\n",
      "Epoch 12/15\n",
      "35233/35233 [==============================] - 130s 4ms/step - loss: 0.4395 - acc: 0.8184 - val_loss: 0.9459 - val_acc: 0.6539\n",
      "Epoch 13/15\n",
      "35233/35233 [==============================] - 130s 4ms/step - loss: 0.4316 - acc: 0.8231 - val_loss: 1.0453 - val_acc: 0.6532\n",
      "Epoch 14/15\n",
      "35233/35233 [==============================] - 130s 4ms/step - loss: 0.4274 - acc: 0.8242 - val_loss: 0.9609 - val_acc: 0.6504\n",
      "Epoch 15/15\n",
      "35233/35233 [==============================] - 133s 4ms/step - loss: 0.4256 - acc: 0.8244 - val_loss: 0.9889 - val_acc: 0.6513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3cc440ef28>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, validation_data=(X_val, y_val), epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-derectional LSTM x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T20:19:53.195177Z",
     "start_time": "2018-10-31T19:36:32.457074Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35233 samples, validate on 15100 samples\n",
      "Epoch 1/20\n",
      "35233/35233 [==============================] - 133s 4ms/step - loss: 0.8855 - acc: 0.5888 - val_loss: 0.7555 - val_acc: 0.6511\n",
      "Epoch 2/20\n",
      "35233/35233 [==============================] - 131s 4ms/step - loss: 0.7755 - acc: 0.6376 - val_loss: 0.7467 - val_acc: 0.6560\n",
      "Epoch 3/20\n",
      "35233/35233 [==============================] - 128s 4ms/step - loss: 0.7400 - acc: 0.6575 - val_loss: 0.7229 - val_acc: 0.6686\n",
      "Epoch 4/20\n",
      "35233/35233 [==============================] - 131s 4ms/step - loss: 0.7124 - acc: 0.6727 - val_loss: 0.7285 - val_acc: 0.6725\n",
      "Epoch 5/20\n",
      "35233/35233 [==============================] - 129s 4ms/step - loss: 0.6925 - acc: 0.6867 - val_loss: 0.7240 - val_acc: 0.6644\n",
      "Epoch 6/20\n",
      "35233/35233 [==============================] - 131s 4ms/step - loss: 0.6774 - acc: 0.6938 - val_loss: 0.7288 - val_acc: 0.6676\n",
      "Epoch 7/20\n",
      "35233/35233 [==============================] - 122s 3ms/step - loss: 0.6579 - acc: 0.7035 - val_loss: 0.7342 - val_acc: 0.6681\n",
      "Epoch 8/20\n",
      "35233/35233 [==============================] - 128s 4ms/step - loss: 0.6488 - acc: 0.7106 - val_loss: 0.7537 - val_acc: 0.6655\n",
      "Epoch 9/20\n",
      "35233/35233 [==============================] - 130s 4ms/step - loss: 0.6424 - acc: 0.7128 - val_loss: 0.7414 - val_acc: 0.6607\n",
      "Epoch 10/20\n",
      "35233/35233 [==============================] - 129s 4ms/step - loss: 0.6291 - acc: 0.7222 - val_loss: 0.7433 - val_acc: 0.6617\n",
      "Epoch 11/20\n",
      "35233/35233 [==============================] - 131s 4ms/step - loss: 0.6194 - acc: 0.7259 - val_loss: 0.7363 - val_acc: 0.6631\n",
      "Epoch 12/20\n",
      "35233/35233 [==============================] - 129s 4ms/step - loss: 0.6008 - acc: 0.7366 - val_loss: 0.7430 - val_acc: 0.6657\n",
      "Epoch 13/20\n",
      "35233/35233 [==============================] - 130s 4ms/step - loss: 0.5938 - acc: 0.7382 - val_loss: 0.7485 - val_acc: 0.6595\n",
      "Epoch 14/20\n",
      "35233/35233 [==============================] - 130s 4ms/step - loss: 0.5853 - acc: 0.7436 - val_loss: 0.7622 - val_acc: 0.6644\n",
      "Epoch 15/20\n",
      "35233/35233 [==============================] - 131s 4ms/step - loss: 0.5749 - acc: 0.7498 - val_loss: 0.7486 - val_acc: 0.6655\n",
      "Epoch 16/20\n",
      "35233/35233 [==============================] - 128s 4ms/step - loss: 0.5637 - acc: 0.7566 - val_loss: 0.7634 - val_acc: 0.6597\n",
      "Epoch 17/20\n",
      "35233/35233 [==============================] - 133s 4ms/step - loss: 0.5560 - acc: 0.7604 - val_loss: 0.7639 - val_acc: 0.6631\n",
      "Epoch 18/20\n",
      "35233/35233 [==============================] - 132s 4ms/step - loss: 0.5513 - acc: 0.7618 - val_loss: 0.7610 - val_acc: 0.6663\n",
      "Epoch 19/20\n",
      "35233/35233 [==============================] - 131s 4ms/step - loss: 0.5395 - acc: 0.7670 - val_loss: 0.7759 - val_acc: 0.6575\n",
      "Epoch 20/20\n",
      "35233/35233 [==============================] - 129s 4ms/step - loss: 0.5302 - acc: 0.7721 - val_loss: 0.7810 - val_acc: 0.6540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3fac06d550>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, validation_data=(X_val, y_val), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T21:33:38.790775Z",
     "start_time": "2018-10-20T21:33:37.153969Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "\n",
    "X = cv.fit_transform(tweets['Tweet'])\n",
    "target = tweets['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T21:33:54.361787Z",
     "start_time": "2018-10-20T21:33:38.793650Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.6240464081373173\n",
      "Accuracy for C=0.05: 0.6523363000635728\n",
      "Accuracy for C=0.25: 0.6594882390336936\n",
      "Accuracy for C=0.5: 0.6562301335028607\n",
      "Accuracy for C=1: 0.6511443102352193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, target)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    lr = LogisticRegression(C=c, solver='saga', multi_class='auto')\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Best and worst features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T21:35:18.047652Z",
     "start_time": "2018-10-20T21:35:17.534418Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('gifford', -1.2377102595086205)\n",
      "('seinfeld', -1.208227717966838)\n",
      "('bout', -1.121596723479545)\n",
      "('bless', -1.1170007079307602)\n",
      "('greatest', -1.0976803765492371)\n",
      "\n",
      "('worst', 2.4448066170437612)\n",
      "('sucks', 2.221116192138116)\n",
      "('stupid', 2.2174777929515206)\n",
      "('fucked', 2.1713174119575873)\n",
      "('fuck', 2.127179056507483)\n"
     ]
    }
   ],
   "source": [
    "feature_to_coef = {word: coef for word, coef in zip(cv.get_feature_names(), lr.coef_[0])}\n",
    "\n",
    "for best_positive in sorted(feature_to_coef.items(), key=lambda x: x[1])[:5]:\n",
    "    print (best_positive)\n",
    "    \n",
    "print()\n",
    "for best_negative in sorted(feature_to_coef.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print (best_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning (pseudo-code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T18:58:26.973787Z",
     "start_time": "2018-10-20T18:58:26.966674Z"
    }
   },
   "outputs": [],
   "source": [
    "Sequential()\n",
    "Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "LSTM(32)\n",
    "Dropout(0.2)\n",
    "Dense(32, 'relu')\n",
    "Dropout(0.2)\n",
    "Dense(3, activation='softmax')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "\n",
    "model.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T18:58:24.330024Z",
     "start_time": "2018-10-20T18:58:24.280540Z"
    }
   },
   "outputs": [],
   "source": [
    "# transfer:\n",
    "\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "Dense(150, 'relu')\n",
    "Dense(64, 'relu')\n",
    "Dense(7, 'softmax')\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "* Télécharger 50 millions de tweets pour le word2vec\n",
    "* Metric pour le script du prof: pearson\n",
    "* Utiliser pearson au lieu d'accuracy\n",
    "* L'année dernière le prof a eu 78% accuracy \n",
    "\n",
    "**Output attendu**: produire un fichier avec les mêmes noms de cartégories sur un fichier de test façon kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
