{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we present our training environments, explain the technical issues we faced, how we tackled them, and finally we launch a large scale training of the word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We tested different techniques to fetch a sufficient dataset of tweets:\n",
    " - Use the twitter streaming API with the Tweepy python wrapper\n",
    " - Use twitterscraper python module to scrap tweets directly from Twitter pages\n",
    " - Download tweets from different sources available on the WEB.\n",
    "\n",
    "Our constraints are the following:\n",
    " - We need more than 30 million tweets, so the fetch technique must be fast enough\n",
    " - We want to limit the number of truncated tweet (i.e. tweets finished by ... cutting a sentence). This is important because training the word2vec embeddings take into account the neighbors of each word.\n",
    " - The language must be english\n",
    "\n",
    "## Use twitter streaming API\n",
    "\n",
    "This technique has been proved to be too slow to be usable in this project. Only one stream can be launched by IP address. This stream can only fetch less than 5 tweets per seconds, and rely a lot on the query words used. There is no need to perform the calculation of the compute time, the constraints are too strong.\n",
    "\n",
    "## Use a twitter WEB scraper\n",
    "\n",
    "This technique is way faster than the previous one. Our script using this technique is able to fetch a stable rate of 20 tweets per seconds, by batches of 800 tweets. It could be improved to perform the fetch part and the database writes in parallel, roughly improving the performances by an expected 50% in the best case.\n",
    "\n",
    "Still, this technique is too slow. It would take 470h of compute time for one worker to fetch the entire dataset, which is more than the time we have for the project. Even considering the previous improvements and that each of the 3 group member could run one worker in parallel, this technique is too expensive in compute time. Moreover the CPU usage is high when using the scraper, forbidding any cloud deployment due to prohibitive cost.\n",
    "\n",
    "## Download a dataset\n",
    "\n",
    "This is the most efficient way to fetch tweets we could afford. There are still some issues with the quality of the dataset. The entire class spent a lot of time to find a large enough dataset matching our constraints. We managed to find what we needed at : https://archive.org/details/archiveteam-twitter-stream-2017-11\n",
    "\n",
    "One can find the little preprocessing we performed on the dataset before inserting into our mysql database in the file \"decompress_dataset.py\"\n",
    "\n",
    "## Ensuring tweet unicity\n",
    "\n",
    "As mentioned before, we used a mysql database to store our tweets temporarily. This has two main objectives :\n",
    " - Low memory usage\n",
    " - Low disk usage\n",
    " - Low cost unicity check\n",
    " - Quite performant data access\n",
    "\n",
    "The unicity of each tweets is checked using the unique tweet id.\n",
    "\n",
    "The only drawback of the usage of a local database is that only the member of the group possessing it can access it 24/24h. We plan to push the data in a large json file on S3 when the dataset is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming from/to the database\n",
    "\n",
    "The code to use the mysql database can be found in mysql_utils.py.\n",
    "\n",
    "By default it uses the mysql database configured on my personnal machine.\n",
    "\n",
    "Print the 10 first tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8951, 'looking for some deep cry for help in the song ease on down the road, but not finding it. it truly is a happy song. damn dorothy and toto ruining my fun.')\n",
      "(9594, 'updating the postmarks project..')\n",
      "(9606, \"my krissy behind it's fine all of the time.\")\n",
      "(9607, \"It would be impossible to surf Linda Mar with the short board, but it won't stop teh Stewie!\")\n",
      "(9618, 'wondering when my conversation will be light hearted again...')\n",
      "(9619, 'Havin a drink at the 500 club in the mission -- to the sound of... oooo the israelites ya')\n",
      "(9626, 'At the yacht club, talking to the bartenders about Wes')\n",
      "(9639, 'Just made up the Deadwood Drinking Game. ')\n",
      "(9644, \"Limon in the Mission is tart, cool and refreshing. Like ceviche? You'll like Limon. Try sauvignon blanc with as an accent :)\")\n",
      "(9645, 'Is that a software architect, enterprise architect, or the real-world kind?')\n"
     ]
    }
   ],
   "source": [
    "from mysql_utils import mysql_reader\n",
    "\n",
    "for tweet in mysql_reader(max=10):\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to push the tweet with id 9619 again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors during insertion: 0\n",
      "Number of tweets in the input stream: 1\n",
      "Number of effectively inserted tweets: 0\n"
     ]
    }
   ],
   "source": [
    "from mysql_utils import Tweet, mysql_sink       \n",
    "\n",
    "tweets = [Tweet(9619, 'Havin a drink at the 500 club in the mission -- to the sound of... oooo the israelites ya')]\n",
    "\n",
    "errors, inserted, stream_size = mysql_sink(iter(tweets))\n",
    "\n",
    "print('Errors during insertion: {}'.format(errors))\n",
    "print('Number of tweets in the input stream: {}'.format(stream_size))\n",
    "print('Number of effectively inserted tweets: {}'.format(inserted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline\n",
    "\n",
    "To simplify the implementation and modification of a preprocessing pipeline of tweets, we implemented some classes to model this pipeline. The base classes can be found in the file \"processing_pipeline.py\".\n",
    "\n",
    "In this section we'll focus on the standardisation part:\n",
    " - Tolenization\n",
    " - Lemmatisation\n",
    " - Stemming\n",
    "\n",
    "Our preprocessing wrappers are available under \"text_preprocessing.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from processing_pipeline import Pipeline\n",
    "from text_preprocessing import TweetTokenizer, NLTKStemmer, NLTKLemmatizer, CorpusWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['look', 'for', 'some', 'deep', 'cri', 'for', 'help', 'in', 'the', 'song', 'eas', 'on', 'down', 'the', 'road', ',', 'but', 'not', 'find', 'it', '.', 'it', 'truli', 'be', 'a', 'happi', 'song', '.', 'damn', 'dorothi', 'and', 'toto', 'ruin', 'my', 'fun', '.']\n",
      "['updat', 'the', 'postmark', 'project', '..']\n",
      "['my', 'krissi', 'behind', \"it'\", 'fine', 'all', 'of', 'the', 'time', '.']\n",
      "['it', 'would', 'be', 'imposs', 'to', 'surf', 'linda', 'mar', 'with', 'the', 'short', 'board', ',', 'but', 'it', \"won't\", 'stop', 'teh', 'stewi', '!']\n",
      "['wonder', 'when', 'my', 'convers', 'will', 'be', 'light', 'heart', 'again', '...']\n",
      "['havin', 'a', 'drink', 'at', 'the', '500', 'club', 'in', 'the', 'mission', '-', '-', 'to', 'the', 'sound', 'of', '...', 'oooo', 'the', 'israelit', 'ya']\n",
      "['at', 'the', 'yacht', 'club', ',', 'talk', 'to', 'the', 'bartend', 'about', 'we']\n",
      "['just', 'make', 'up', 'the', 'deadwood', 'drink', 'game', '.']\n",
      "['limon', 'in', 'the', 'mission', 'be', 'tart', ',', 'cool', 'and', 'refresh', '.', 'like', 'cevich', '?', \"you'll\", 'like', 'limon', '.', 'tri', 'sauvignon', 'blanc', 'with', 'as', 'an', 'accent', ':)']\n",
      "['be', 'that', 'a', 'softwar', 'architect', ',', 'enterpris', 'architect', ',', 'or', 'the', 'real-world', 'kind', '?']\n"
     ]
    }
   ],
   "source": [
    "preprocessor_factories = [\n",
    "    TweetTokenizer,\n",
    "    lambda tokens: CorpusWrapper(NLTKStemmer, tokens),\n",
    "    lambda tokens: CorpusWrapper(NLTKLemmatizer, tokens),\n",
    "]\n",
    "\n",
    "test_raw_tweet_stream = mysql_reader(max=10)\n",
    "test_tweet_text_stream = map(lambda tweet: tweet[1], test_raw_tweet_stream)\n",
    "\n",
    "pipeline = Pipeline(test_tweet_text_stream, preprocessor_factories)\n",
    "\n",
    "for val in pipeline:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large scale training\n",
    "\n",
    "In order to use gensim word2vec implementation, we need to provide batches of tokenized sentences. We use batches of 10000 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial                  # Nicer than lambdas\n",
    "from database_to_json import read_tweet_json   # Nicer than maps\n",
    "from processing_pipeline import BatchMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_stream = read_tweet_json(max=23563755)\n",
    "\n",
    "factories = [\n",
    "    TweetTokenizer,\n",
    "    partial(CorpusWrapper, NLTKStemmer),\n",
    "    partial(CorpusWrapper, NLTKLemmatizer),\n",
    "    partial(BatchMaker, batch_size=100000),\n",
    "]\n",
    "\n",
    "batch_pipeline = Pipeline(input_stream, factories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the pipeline for hashtags and twitter users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#tata']\n",
      "['@toto']\n"
     ]
    }
   ],
   "source": [
    "factories = [\n",
    "    TweetTokenizer,\n",
    "    partial(CorpusWrapper, NLTKStemmer),\n",
    "    partial(CorpusWrapper, NLTKLemmatizer),\n",
    "    partial(BatchMaker, batch_size=100000),\n",
    "]\n",
    "\n",
    "test = ['#tata', '@toto']\n",
    "test_pip = Pipeline(test, factories)\n",
    "\n",
    "for b in test_pip:\n",
    "    for w in b:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: it's ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training at: 2018-11-13 08:02:50.244557\n",
      "Start batch n°1 at 2018-11-13 08:02:50.247117\n",
      "End of batch n°1 at 2018-11-13 08:05:54.121536\n",
      "Start batch n°2 at 2018-11-13 08:05:54.121763\n",
      "End of batch n°2 at 2018-11-13 08:07:58.304526\n",
      "Start batch n°3 at 2018-11-13 08:07:58.304669\n",
      "End of batch n°3 at 2018-11-13 08:09:51.643927\n",
      "Start batch n°4 at 2018-11-13 08:09:51.644050\n",
      "End of batch n°4 at 2018-11-13 08:11:37.922917\n",
      "Start batch n°5 at 2018-11-13 08:11:37.923267\n",
      "End of batch n°5 at 2018-11-13 08:13:27.317980\n",
      "Start batch n°6 at 2018-11-13 08:13:27.318124\n",
      "End of batch n°6 at 2018-11-13 08:15:20.384651\n",
      "Start batch n°7 at 2018-11-13 08:15:20.384844\n",
      "End of batch n°7 at 2018-11-13 08:16:54.683148\n",
      "Start batch n°8 at 2018-11-13 08:16:54.683313\n",
      "End of batch n°8 at 2018-11-13 08:18:31.939057\n",
      "Start batch n°9 at 2018-11-13 08:18:31.939227\n",
      "End of batch n°9 at 2018-11-13 08:20:08.517673\n",
      "Start batch n°10 at 2018-11-13 08:20:08.517798\n",
      "End of batch n°10 at 2018-11-13 08:21:40.700399\n",
      "Start batch n°11 at 2018-11-13 08:21:40.701206\n",
      "End of batch n°11 at 2018-11-13 08:23:13.086650\n",
      "Start batch n°12 at 2018-11-13 08:23:13.086786\n",
      "End of batch n°12 at 2018-11-13 08:24:49.177641\n",
      "Start batch n°13 at 2018-11-13 08:24:49.177795\n",
      "End of batch n°13 at 2018-11-13 08:26:28.068071\n",
      "Start batch n°14 at 2018-11-13 08:26:28.068195\n",
      "End of batch n°14 at 2018-11-13 08:28:06.547350\n",
      "Start batch n°15 at 2018-11-13 08:28:06.547490\n",
      "End of batch n°15 at 2018-11-13 08:29:42.018615\n",
      "Start batch n°16 at 2018-11-13 08:29:42.018757\n",
      "End of batch n°16 at 2018-11-13 08:31:17.003178\n",
      "Start batch n°17 at 2018-11-13 08:31:17.003639\n",
      "End of batch n°17 at 2018-11-13 08:32:55.186810\n",
      "Start batch n°18 at 2018-11-13 08:32:55.186962\n",
      "End of batch n°18 at 2018-11-13 08:34:34.074416\n",
      "Start batch n°19 at 2018-11-13 08:34:34.074567\n",
      "End of batch n°19 at 2018-11-13 08:36:13.640047\n",
      "Start batch n°20 at 2018-11-13 08:36:13.640216\n",
      "End of batch n°20 at 2018-11-13 08:37:48.369676\n",
      "Start batch n°21 at 2018-11-13 08:37:48.370116\n",
      "End of batch n°21 at 2018-11-13 08:39:23.716810\n",
      "Start batch n°22 at 2018-11-13 08:39:23.717031\n",
      "End of batch n°22 at 2018-11-13 08:40:59.670439\n",
      "Start batch n°23 at 2018-11-13 08:40:59.670896\n",
      "End of batch n°23 at 2018-11-13 08:42:36.666622\n",
      "Start batch n°24 at 2018-11-13 08:42:36.666759\n",
      "End of batch n°24 at 2018-11-13 08:44:10.747807\n",
      "Start batch n°25 at 2018-11-13 08:44:10.747945\n",
      "End of batch n°25 at 2018-11-13 08:45:43.681893\n",
      "Start batch n°26 at 2018-11-13 08:45:43.682244\n",
      "End of batch n°26 at 2018-11-13 08:47:20.298974\n",
      "Start batch n°27 at 2018-11-13 08:47:20.299118\n",
      "End of batch n°27 at 2018-11-13 08:48:51.644122\n",
      "Start batch n°28 at 2018-11-13 08:48:51.644596\n",
      "End of batch n°28 at 2018-11-13 08:50:23.689446\n",
      "Start batch n°29 at 2018-11-13 08:50:23.689632\n",
      "End of batch n°29 at 2018-11-13 08:51:56.492154\n",
      "Start batch n°30 at 2018-11-13 08:51:56.492290\n",
      "End of batch n°30 at 2018-11-13 08:53:32.492001\n",
      "Start batch n°31 at 2018-11-13 08:53:32.492137\n",
      "End of batch n°31 at 2018-11-13 08:55:12.886127\n",
      "Start batch n°32 at 2018-11-13 08:55:12.886614\n",
      "End of batch n°32 at 2018-11-13 08:56:54.105835\n",
      "Start batch n°33 at 2018-11-13 08:56:54.105991\n",
      "End of batch n°33 at 2018-11-13 08:58:30.949156\n",
      "Start batch n°34 at 2018-11-13 08:58:30.949304\n",
      "End of batch n°34 at 2018-11-13 09:00:07.078347\n",
      "Start batch n°35 at 2018-11-13 09:00:07.078469\n",
      "End of batch n°35 at 2018-11-13 09:01:44.832180\n",
      "Start batch n°36 at 2018-11-13 09:01:44.832652\n",
      "End of batch n°36 at 2018-11-13 09:03:25.108731\n",
      "Start batch n°37 at 2018-11-13 09:03:25.108870\n",
      "End of batch n°37 at 2018-11-13 09:04:31.142689\n",
      "Start batch n°38 at 2018-11-13 09:04:31.142806\n",
      "End of batch n°38 at 2018-11-13 09:05:40.643654\n",
      "Start batch n°39 at 2018-11-13 09:05:40.643771\n",
      "End of batch n°39 at 2018-11-13 09:06:48.963157\n",
      "Start batch n°40 at 2018-11-13 09:06:48.963276\n",
      "End of batch n°40 at 2018-11-13 09:07:58.311386\n",
      "Start batch n°41 at 2018-11-13 09:07:58.311506\n",
      "End of batch n°41 at 2018-11-13 09:09:09.024141\n",
      "Start batch n°42 at 2018-11-13 09:09:09.024287\n",
      "End of batch n°42 at 2018-11-13 09:10:18.789665\n",
      "Start batch n°43 at 2018-11-13 09:10:18.789824\n",
      "End of batch n°43 at 2018-11-13 09:11:26.763164\n",
      "Start batch n°44 at 2018-11-13 09:11:26.763282\n",
      "End of batch n°44 at 2018-11-13 09:12:34.789456\n",
      "Start batch n°45 at 2018-11-13 09:12:34.789589\n",
      "End of batch n°45 at 2018-11-13 09:13:51.609528\n",
      "Start batch n°46 at 2018-11-13 09:13:51.609650\n",
      "End of batch n°46 at 2018-11-13 09:15:13.913773\n",
      "Start batch n°47 at 2018-11-13 09:15:13.913905\n",
      "End of batch n°47 at 2018-11-13 09:16:35.007399\n",
      "Start batch n°48 at 2018-11-13 09:16:35.007520\n",
      "End of batch n°48 at 2018-11-13 09:17:55.565407\n",
      "Start batch n°49 at 2018-11-13 09:17:55.565546\n",
      "End of batch n°49 at 2018-11-13 09:19:17.528999\n",
      "Start batch n°50 at 2018-11-13 09:19:17.529150\n",
      "End of batch n°50 at 2018-11-13 09:20:42.970279\n",
      "Start batch n°51 at 2018-11-13 09:20:42.970420\n",
      "End of batch n°51 at 2018-11-13 09:22:06.715545\n",
      "Start batch n°52 at 2018-11-13 09:22:06.715667\n",
      "End of batch n°52 at 2018-11-13 09:23:30.458010\n",
      "Start batch n°53 at 2018-11-13 09:23:30.458143\n",
      "End of batch n°53 at 2018-11-13 09:24:52.996425\n",
      "Start batch n°54 at 2018-11-13 09:24:52.996548\n",
      "End of batch n°54 at 2018-11-13 09:26:15.359953\n",
      "Start batch n°55 at 2018-11-13 09:26:15.360074\n",
      "End of batch n°55 at 2018-11-13 09:27:38.379953\n",
      "Start batch n°56 at 2018-11-13 09:27:38.380092\n",
      "End of batch n°56 at 2018-11-13 09:28:58.849910\n",
      "Start batch n°57 at 2018-11-13 09:28:58.850030\n",
      "End of batch n°57 at 2018-11-13 09:30:18.231813\n",
      "Start batch n°58 at 2018-11-13 09:30:18.231933\n",
      "End of batch n°58 at 2018-11-13 09:31:40.649093\n",
      "Start batch n°59 at 2018-11-13 09:31:40.649232\n",
      "End of batch n°59 at 2018-11-13 09:33:02.010933\n",
      "Start batch n°60 at 2018-11-13 09:33:02.011055\n",
      "End of batch n°60 at 2018-11-13 09:34:24.041508\n",
      "Start batch n°61 at 2018-11-13 09:34:24.041644\n",
      "End of batch n°61 at 2018-11-13 09:35:45.809700\n",
      "Start batch n°62 at 2018-11-13 09:35:45.809826\n",
      "End of batch n°62 at 2018-11-13 09:37:11.925690\n",
      "Start batch n°63 at 2018-11-13 09:37:11.925836\n",
      "End of batch n°63 at 2018-11-13 09:38:41.200286\n",
      "Start batch n°64 at 2018-11-13 09:38:41.200415\n",
      "End of batch n°64 at 2018-11-13 09:40:10.207858\n",
      "Start batch n°65 at 2018-11-13 09:40:10.208097\n",
      "End of batch n°65 at 2018-11-13 09:41:36.631123\n",
      "Start batch n°66 at 2018-11-13 09:41:36.631252\n",
      "End of batch n°66 at 2018-11-13 09:42:59.920985\n",
      "Start batch n°67 at 2018-11-13 09:42:59.921104\n",
      "End of batch n°67 at 2018-11-13 09:44:19.659070\n",
      "Start batch n°68 at 2018-11-13 09:44:19.659187\n",
      "End of batch n°68 at 2018-11-13 09:45:40.998609\n",
      "Start batch n°69 at 2018-11-13 09:45:40.998734\n",
      "End of batch n°69 at 2018-11-13 09:47:02.994361\n",
      "Start batch n°70 at 2018-11-13 09:47:02.994483\n",
      "End of batch n°70 at 2018-11-13 09:48:23.318435\n",
      "Start batch n°71 at 2018-11-13 09:48:23.318560\n",
      "End of batch n°71 at 2018-11-13 09:49:46.209780\n",
      "Start batch n°72 at 2018-11-13 09:49:46.209919\n",
      "End of batch n°72 at 2018-11-13 09:51:11.499303\n",
      "Start batch n°73 at 2018-11-13 09:51:11.499438\n",
      "End of batch n°73 at 2018-11-13 09:52:40.655270\n",
      "Start batch n°74 at 2018-11-13 09:52:40.655413\n",
      "End of batch n°74 at 2018-11-13 09:54:11.166350\n",
      "Start batch n°75 at 2018-11-13 09:54:11.166520\n",
      "End of batch n°75 at 2018-11-13 09:55:40.587952\n",
      "Start batch n°76 at 2018-11-13 09:55:40.588104\n",
      "End of batch n°76 at 2018-11-13 09:57:10.248892\n",
      "Start batch n°77 at 2018-11-13 09:57:10.249030\n",
      "End of batch n°77 at 2018-11-13 09:58:43.392414\n",
      "Start batch n°78 at 2018-11-13 09:58:43.392535\n",
      "End of batch n°78 at 2018-11-13 10:00:14.715349\n",
      "Start batch n°79 at 2018-11-13 10:00:14.715493\n",
      "End of batch n°79 at 2018-11-13 10:01:43.867742\n",
      "Start batch n°80 at 2018-11-13 10:01:43.867868\n",
      "End of batch n°80 at 2018-11-13 10:03:11.626397\n",
      "Start batch n°81 at 2018-11-13 10:03:11.626542\n",
      "End of batch n°81 at 2018-11-13 10:04:44.862469\n",
      "Start batch n°82 at 2018-11-13 10:04:44.862614\n",
      "End of batch n°82 at 2018-11-13 10:06:18.132455\n",
      "Start batch n°83 at 2018-11-13 10:06:18.132595\n",
      "End of batch n°83 at 2018-11-13 10:07:49.682239\n",
      "Start batch n°84 at 2018-11-13 10:07:49.682366\n",
      "End of batch n°84 at 2018-11-13 10:09:18.348038\n",
      "Start batch n°85 at 2018-11-13 10:09:18.348190\n",
      "End of batch n°85 at 2018-11-13 10:10:49.213710\n",
      "Start batch n°86 at 2018-11-13 10:10:49.213840\n",
      "End of batch n°86 at 2018-11-13 10:12:20.241388\n",
      "Start batch n°87 at 2018-11-13 10:12:20.241509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of batch n°87 at 2018-11-13 10:13:57.548798\n",
      "Start batch n°88 at 2018-11-13 10:13:57.548958\n",
      "End of batch n°88 at 2018-11-13 10:15:37.159494\n",
      "Start batch n°89 at 2018-11-13 10:15:37.159630\n",
      "End of batch n°89 at 2018-11-13 10:17:10.605119\n",
      "Start batch n°90 at 2018-11-13 10:17:10.605262\n",
      "End of batch n°90 at 2018-11-13 10:18:46.589226\n",
      "Start batch n°91 at 2018-11-13 10:18:46.589376\n",
      "End of batch n°91 at 2018-11-13 10:20:26.650993\n",
      "Start batch n°92 at 2018-11-13 10:20:26.651138\n",
      "End of batch n°92 at 2018-11-13 10:22:06.190088\n",
      "Start batch n°93 at 2018-11-13 10:22:06.190263\n",
      "End of batch n°93 at 2018-11-13 10:23:30.716949\n",
      "Start batch n°94 at 2018-11-13 10:23:30.717065\n",
      "End of batch n°94 at 2018-11-13 10:24:57.830386\n",
      "Start batch n°95 at 2018-11-13 10:24:57.830939\n",
      "End of batch n°95 at 2018-11-13 10:26:28.399595\n",
      "Start batch n°96 at 2018-11-13 10:26:28.399727\n",
      "End of batch n°96 at 2018-11-13 10:27:43.892722\n",
      "Start batch n°97 at 2018-11-13 10:27:43.892837\n",
      "End of batch n°97 at 2018-11-13 10:29:01.624618\n",
      "Start batch n°98 at 2018-11-13 10:29:01.624738\n",
      "End of batch n°98 at 2018-11-13 10:30:13.875360\n",
      "Start batch n°99 at 2018-11-13 10:30:13.875493\n",
      "End of batch n°99 at 2018-11-13 10:31:23.334980\n",
      "Start batch n°100 at 2018-11-13 10:31:23.335099\n",
      "End of batch n°100 at 2018-11-13 10:32:41.531880\n",
      "Start batch n°101 at 2018-11-13 10:32:41.532036\n",
      "End of batch n°101 at 2018-11-13 10:34:03.788500\n",
      "Start batch n°102 at 2018-11-13 10:34:03.788632\n",
      "End of batch n°102 at 2018-11-13 10:35:24.267991\n",
      "Start batch n°103 at 2018-11-13 10:35:24.268140\n",
      "End of batch n°103 at 2018-11-13 10:36:36.874110\n",
      "Start batch n°104 at 2018-11-13 10:36:36.874227\n",
      "End of batch n°104 at 2018-11-13 10:37:50.819568\n",
      "Start batch n°105 at 2018-11-13 10:37:50.819935\n",
      "End of batch n°105 at 2018-11-13 10:39:37.546309\n",
      "Start batch n°106 at 2018-11-13 10:39:38.581008\n",
      "End of batch n°106 at 2018-11-13 10:42:08.354172\n",
      "Start batch n°107 at 2018-11-13 10:42:08.354735\n",
      "End of batch n°107 at 2018-11-13 10:43:42.746469\n",
      "Start batch n°108 at 2018-11-13 10:43:42.746890\n",
      "End of batch n°108 at 2018-11-13 10:45:21.434660\n",
      "Start batch n°109 at 2018-11-13 10:45:21.435099\n",
      "End of batch n°109 at 2018-11-13 10:47:14.762540\n",
      "Start batch n°110 at 2018-11-13 10:47:14.762747\n",
      "End of batch n°110 at 2018-11-13 10:49:24.921277\n",
      "Start batch n°111 at 2018-11-13 10:49:24.921766\n",
      "End of batch n°111 at 2018-11-13 10:51:24.920464\n",
      "Start batch n°112 at 2018-11-13 10:51:24.920635\n",
      "End of batch n°112 at 2018-11-13 10:53:25.527751\n",
      "Start batch n°113 at 2018-11-13 10:53:25.527898\n",
      "End of batch n°113 at 2018-11-13 10:55:19.924952\n",
      "Start batch n°114 at 2018-11-13 10:55:19.925099\n",
      "End of batch n°114 at 2018-11-13 10:57:19.715083\n",
      "Start batch n°115 at 2018-11-13 10:57:19.715446\n",
      "End of batch n°115 at 2018-11-13 10:59:22.511696\n",
      "Start batch n°116 at 2018-11-13 10:59:22.511917\n",
      "End of batch n°116 at 2018-11-13 11:01:23.797383\n",
      "Start batch n°117 at 2018-11-13 11:01:23.797576\n",
      "End of batch n°117 at 2018-11-13 11:03:20.178470\n",
      "Start batch n°118 at 2018-11-13 11:03:20.178675\n",
      "End of batch n°118 at 2018-11-13 11:05:33.864622\n",
      "Start batch n°119 at 2018-11-13 11:05:33.864905\n",
      "End of batch n°119 at 2018-11-13 11:07:52.399169\n",
      "Start batch n°120 at 2018-11-13 11:07:52.399660\n",
      "End of batch n°120 at 2018-11-13 11:10:11.909665\n",
      "Start batch n°121 at 2018-11-13 11:10:11.909829\n",
      "End of batch n°121 at 2018-11-13 11:12:22.970241\n",
      "Start batch n°122 at 2018-11-13 11:12:22.970377\n",
      "End of batch n°122 at 2018-11-13 11:14:21.804875\n",
      "Start batch n°123 at 2018-11-13 11:14:21.805347\n",
      "End of batch n°123 at 2018-11-13 11:16:18.789639\n",
      "Start batch n°124 at 2018-11-13 11:16:18.789782\n",
      "End of batch n°124 at 2018-11-13 11:18:18.688182\n",
      "Start batch n°125 at 2018-11-13 11:18:18.688304\n",
      "End of batch n°125 at 2018-11-13 11:20:14.247264\n",
      "Start batch n°126 at 2018-11-13 11:20:14.247400\n",
      "End of batch n°126 at 2018-11-13 11:22:03.872200\n",
      "Start batch n°127 at 2018-11-13 11:22:03.872362\n",
      "End of batch n°127 at 2018-11-13 11:23:56.301936\n",
      "Start batch n°128 at 2018-11-13 11:23:56.302409\n",
      "End of batch n°128 at 2018-11-13 11:25:51.547745\n",
      "Start batch n°129 at 2018-11-13 11:25:51.547915\n",
      "End of batch n°129 at 2018-11-13 11:27:45.748606\n",
      "Start batch n°130 at 2018-11-13 11:27:45.748730\n",
      "End of batch n°130 at 2018-11-13 11:29:38.399263\n",
      "Start batch n°131 at 2018-11-13 11:29:38.399408\n",
      "End of batch n°131 at 2018-11-13 11:31:27.221842\n",
      "Start batch n°132 at 2018-11-13 11:31:27.222021\n",
      "End of batch n°132 at 2018-11-13 11:33:21.235845\n",
      "Start batch n°133 at 2018-11-13 11:33:21.236023\n",
      "End of batch n°133 at 2018-11-13 11:35:19.313102\n",
      "Start batch n°134 at 2018-11-13 11:35:19.313236\n",
      "End of batch n°134 at 2018-11-13 11:37:12.062412\n",
      "Start batch n°135 at 2018-11-13 11:37:12.063103\n",
      "End of batch n°135 at 2018-11-13 11:39:00.386048\n",
      "Start batch n°136 at 2018-11-13 11:39:00.386223\n",
      "End of batch n°136 at 2018-11-13 11:40:52.839137\n",
      "Start batch n°137 at 2018-11-13 11:40:52.839518\n",
      "End of batch n°137 at 2018-11-13 11:42:46.770076\n",
      "Start batch n°138 at 2018-11-13 11:42:46.770211\n",
      "End of batch n°138 at 2018-11-13 11:44:43.839906\n",
      "Start batch n°139 at 2018-11-13 11:44:43.840204\n",
      "End of batch n°139 at 2018-11-13 11:46:32.575548\n",
      "Start batch n°140 at 2018-11-13 11:46:32.575700\n",
      "End of batch n°140 at 2018-11-13 11:48:20.844003\n",
      "Start batch n°141 at 2018-11-13 11:48:20.844128\n",
      "End of batch n°141 at 2018-11-13 11:50:13.051975\n",
      "Start batch n°142 at 2018-11-13 11:50:13.052442\n",
      "End of batch n°142 at 2018-11-13 11:52:12.812573\n",
      "Start batch n°143 at 2018-11-13 11:52:12.812743\n",
      "End of batch n°143 at 2018-11-13 11:54:09.632901\n",
      "Start batch n°144 at 2018-11-13 11:54:09.633406\n",
      "End of batch n°144 at 2018-11-13 11:56:05.546923\n",
      "Start batch n°145 at 2018-11-13 11:56:05.547085\n",
      "End of batch n°145 at 2018-11-13 11:58:01.631992\n",
      "Start batch n°146 at 2018-11-13 11:58:01.632162\n",
      "End of batch n°146 at 2018-11-13 11:59:51.637434\n",
      "Start batch n°147 at 2018-11-13 11:59:51.637561\n",
      "End of batch n°147 at 2018-11-13 12:01:50.646243\n",
      "Start batch n°148 at 2018-11-13 12:01:50.706991\n",
      "End of batch n°148 at 2018-11-13 12:03:46.970085\n",
      "Start batch n°149 at 2018-11-13 12:03:46.970233\n",
      "End of batch n°149 at 2018-11-13 12:05:40.914797\n",
      "Start batch n°150 at 2018-11-13 12:05:40.915272\n",
      "End of batch n°150 at 2018-11-13 12:07:43.382281\n",
      "Start batch n°151 at 2018-11-13 12:07:43.382440\n",
      "End of batch n°151 at 2018-11-13 12:09:39.116413\n",
      "Start batch n°152 at 2018-11-13 12:09:39.116757\n",
      "End of batch n°152 at 2018-11-13 12:11:31.686280\n",
      "Start batch n°153 at 2018-11-13 12:11:31.686408\n",
      "End of batch n°153 at 2018-11-13 12:13:20.677832\n",
      "Start batch n°154 at 2018-11-13 12:13:20.677981\n",
      "End of batch n°154 at 2018-11-13 12:15:14.928839\n",
      "Start batch n°155 at 2018-11-13 12:15:14.929298\n",
      "End of batch n°155 at 2018-11-13 12:17:15.116349\n",
      "Start batch n°156 at 2018-11-13 12:17:15.116485\n",
      "End of batch n°156 at 2018-11-13 12:19:06.237699\n",
      "Start batch n°157 at 2018-11-13 12:19:06.237881\n",
      "End of batch n°157 at 2018-11-13 12:20:55.286555\n",
      "Start batch n°158 at 2018-11-13 12:20:55.287145\n",
      "End of batch n°158 at 2018-11-13 12:22:47.119515\n",
      "Start batch n°159 at 2018-11-13 12:22:47.119705\n",
      "End of batch n°159 at 2018-11-13 12:24:41.040198\n",
      "Start batch n°160 at 2018-11-13 12:24:41.040670\n",
      "End of batch n°160 at 2018-11-13 12:26:34.222987\n",
      "Start batch n°161 at 2018-11-13 12:26:34.223125\n",
      "End of batch n°161 at 2018-11-13 12:28:27.480968\n",
      "Start batch n°162 at 2018-11-13 12:28:27.481099\n",
      "End of batch n°162 at 2018-11-13 12:30:16.119631\n",
      "Start batch n°163 at 2018-11-13 12:30:16.135650\n",
      "End of batch n°163 at 2018-11-13 12:32:08.764103\n",
      "Start batch n°164 at 2018-11-13 12:32:08.764276\n",
      "End of batch n°164 at 2018-11-13 12:34:05.235869\n",
      "Start batch n°165 at 2018-11-13 12:34:05.236041\n",
      "End of batch n°165 at 2018-11-13 12:36:02.257109\n",
      "Start batch n°166 at 2018-11-13 12:36:02.257259\n",
      "End of batch n°166 at 2018-11-13 12:37:56.278160\n",
      "Start batch n°167 at 2018-11-13 12:37:56.278318\n",
      "End of batch n°167 at 2018-11-13 12:39:47.871642\n",
      "Start batch n°168 at 2018-11-13 12:39:47.871844\n",
      "End of batch n°168 at 2018-11-13 12:41:45.089982\n",
      "Start batch n°169 at 2018-11-13 12:41:45.090115\n",
      "End of batch n°169 at 2018-11-13 12:43:43.369440\n",
      "Start batch n°170 at 2018-11-13 12:43:43.369632\n",
      "End of batch n°170 at 2018-11-13 12:45:35.413101\n",
      "Start batch n°171 at 2018-11-13 12:45:35.413287\n",
      "End of batch n°171 at 2018-11-13 12:47:28.071525\n",
      "Start batch n°172 at 2018-11-13 12:47:28.071664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of batch n°172 at 2018-11-13 12:49:19.755835\n",
      "Start batch n°173 at 2018-11-13 12:49:19.756001\n",
      "End of batch n°173 at 2018-11-13 12:51:15.533986\n",
      "Start batch n°174 at 2018-11-13 12:51:15.534117\n",
      "End of batch n°174 at 2018-11-13 12:53:09.277625\n",
      "Start batch n°175 at 2018-11-13 12:53:09.278067\n",
      "End of batch n°175 at 2018-11-13 12:54:59.693679\n",
      "Start batch n°176 at 2018-11-13 12:54:59.693822\n",
      "End of batch n°176 at 2018-11-13 12:56:51.764661\n",
      "Start batch n°177 at 2018-11-13 12:56:51.764790\n",
      "End of batch n°177 at 2018-11-13 12:59:00.527433\n",
      "Start batch n°178 at 2018-11-13 12:59:00.527917\n",
      "End of batch n°178 at 2018-11-13 13:01:06.107741\n",
      "Start batch n°179 at 2018-11-13 13:01:06.108249\n",
      "End of batch n°179 at 2018-11-13 13:03:14.184759\n",
      "Start batch n°180 at 2018-11-13 13:03:14.184943\n",
      "End of batch n°180 at 2018-11-13 13:05:09.774381\n",
      "Start batch n°181 at 2018-11-13 13:05:09.774511\n",
      "End of batch n°181 at 2018-11-13 13:07:07.591642\n",
      "Start batch n°182 at 2018-11-13 13:07:07.591774\n",
      "End of batch n°182 at 2018-11-13 13:09:07.899643\n",
      "Start batch n°183 at 2018-11-13 13:09:07.899780\n",
      "End of batch n°183 at 2018-11-13 13:11:03.221656\n",
      "Start batch n°184 at 2018-11-13 13:11:03.221779\n",
      "End of batch n°184 at 2018-11-13 13:12:54.094233\n",
      "Start batch n°185 at 2018-11-13 13:12:54.094406\n",
      "End of batch n°185 at 2018-11-13 13:14:53.111809\n",
      "Start batch n°186 at 2018-11-13 13:14:53.111952\n",
      "End of batch n°186 at 2018-11-13 13:16:52.541438\n",
      "Start batch n°187 at 2018-11-13 13:16:52.541799\n",
      "End of batch n°187 at 2018-11-13 13:18:47.824003\n",
      "Start batch n°188 at 2018-11-13 13:18:47.824359\n",
      "End of batch n°188 at 2018-11-13 13:20:39.552456\n",
      "Start batch n°189 at 2018-11-13 13:20:39.552648\n",
      "End of batch n°189 at 2018-11-13 13:22:36.419309\n",
      "Start batch n°190 at 2018-11-13 13:22:36.419451\n",
      "End of batch n°190 at 2018-11-13 13:24:35.182328\n",
      "Start batch n°191 at 2018-11-13 13:24:35.182493\n",
      "End of batch n°191 at 2018-11-13 13:26:44.516872\n",
      "Start batch n°192 at 2018-11-13 13:26:44.517004\n",
      "End of batch n°192 at 2018-11-13 13:28:48.305613\n",
      "Start batch n°193 at 2018-11-13 13:28:48.305771\n",
      "End of batch n°193 at 2018-11-13 13:30:47.557509\n",
      "Start batch n°194 at 2018-11-13 13:30:47.557888\n",
      "End of batch n°194 at 2018-11-13 13:32:53.931717\n",
      "Start batch n°195 at 2018-11-13 13:32:53.932273\n",
      "End of batch n°195 at 2018-11-13 13:35:14.758497\n",
      "Start batch n°196 at 2018-11-13 13:35:14.758657\n",
      "End of batch n°196 at 2018-11-13 13:37:28.815680\n",
      "Start batch n°197 at 2018-11-13 13:37:28.815819\n",
      "End of batch n°197 at 2018-11-13 13:39:31.891228\n",
      "Start batch n°198 at 2018-11-13 13:39:31.953446\n",
      "End of batch n°198 at 2018-11-13 13:41:45.993388\n",
      "Start batch n°199 at 2018-11-13 13:41:46.054079\n",
      "End of batch n°199 at 2018-11-13 13:43:43.630638\n",
      "Start batch n°200 at 2018-11-13 13:43:43.630803\n",
      "End of batch n°200 at 2018-11-13 13:45:39.387510\n",
      "Start batch n°201 at 2018-11-13 13:45:39.388011\n",
      "End of batch n°201 at 2018-11-13 13:47:31.643501\n",
      "Start batch n°202 at 2018-11-13 13:47:31.643666\n",
      "End of batch n°202 at 2018-11-13 13:49:52.979638\n",
      "Start batch n°203 at 2018-11-13 13:49:52.980019\n",
      "End of batch n°203 at 2018-11-13 13:51:49.059845\n",
      "Start batch n°204 at 2018-11-13 13:51:49.060277\n",
      "End of batch n°204 at 2018-11-13 13:53:46.626120\n",
      "Start batch n°205 at 2018-11-13 13:53:46.626270\n",
      "End of batch n°205 at 2018-11-13 13:55:38.817156\n",
      "Start batch n°206 at 2018-11-13 13:55:38.817284\n",
      "End of batch n°206 at 2018-11-13 13:57:32.457944\n",
      "Start batch n°207 at 2018-11-13 13:57:32.458440\n",
      "End of batch n°207 at 2018-11-13 13:59:36.166151\n",
      "Start batch n°208 at 2018-11-13 13:59:36.166302\n",
      "End of batch n°208 at 2018-11-13 14:01:37.455117\n",
      "Start batch n°209 at 2018-11-13 14:01:37.455317\n",
      "End of batch n°209 at 2018-11-13 14:03:34.204638\n",
      "Start batch n°210 at 2018-11-13 14:03:34.204798\n",
      "End of batch n°210 at 2018-11-13 14:05:26.595242\n",
      "Start batch n°211 at 2018-11-13 14:05:26.595399\n",
      "End of batch n°211 at 2018-11-13 14:07:19.398391\n",
      "Start batch n°212 at 2018-11-13 14:07:19.398821\n",
      "End of batch n°212 at 2018-11-13 14:09:14.537719\n",
      "Start batch n°213 at 2018-11-13 14:09:14.537866\n",
      "End of batch n°213 at 2018-11-13 14:11:03.190065\n",
      "Start batch n°214 at 2018-11-13 14:11:03.190249\n",
      "End of batch n°214 at 2018-11-13 14:13:00.192848\n",
      "Start batch n°215 at 2018-11-13 14:13:00.193475\n",
      "End of batch n°215 at 2018-11-13 14:14:55.856389\n",
      "Start batch n°216 at 2018-11-13 14:14:55.856556\n",
      "End of batch n°216 at 2018-11-13 14:16:49.673994\n",
      "Start batch n°217 at 2018-11-13 14:16:49.674143\n",
      "End of batch n°217 at 2018-11-13 14:18:39.593412\n",
      "Start batch n°218 at 2018-11-13 14:18:39.593543\n",
      "End of batch n°218 at 2018-11-13 14:20:30.462106\n",
      "Start batch n°219 at 2018-11-13 14:20:30.462549\n",
      "End of batch n°219 at 2018-11-13 14:22:36.210827\n",
      "Start batch n°220 at 2018-11-13 14:22:36.274573\n",
      "End of batch n°220 at 2018-11-13 14:24:35.780465\n",
      "Start batch n°221 at 2018-11-13 14:24:35.948112\n",
      "End of batch n°221 at 2018-11-13 14:26:31.593618\n",
      "Start batch n°222 at 2018-11-13 14:26:31.593781\n",
      "End of batch n°222 at 2018-11-13 14:28:29.183751\n",
      "Start batch n°223 at 2018-11-13 14:28:29.183912\n",
      "End of batch n°223 at 2018-11-13 14:30:34.125372\n",
      "Start batch n°224 at 2018-11-13 14:30:34.125562\n",
      "End of batch n°224 at 2018-11-13 14:32:33.701276\n",
      "Start batch n°225 at 2018-11-13 14:32:33.701445\n",
      "End of batch n°225 at 2018-11-13 14:34:39.200860\n",
      "Start batch n°226 at 2018-11-13 14:34:39.201015\n",
      "End of batch n°226 at 2018-11-13 14:36:35.577311\n",
      "Start batch n°227 at 2018-11-13 14:36:35.577472\n",
      "End of batch n°227 at 2018-11-13 14:38:33.455598\n",
      "Start batch n°228 at 2018-11-13 14:38:33.455751\n",
      "End of batch n°228 at 2018-11-13 14:40:35.611011\n",
      "Start batch n°229 at 2018-11-13 14:40:35.611137\n",
      "End of batch n°229 at 2018-11-13 14:47:21.572832\n",
      "Start batch n°230 at 2018-11-13 14:47:21.820694\n",
      "End of batch n°230 at 2018-11-13 14:49:13.900626\n",
      "Start batch n°231 at 2018-11-13 14:49:13.900789\n",
      "End of batch n°231 at 2018-11-13 14:51:07.758946\n",
      "Start batch n°232 at 2018-11-13 14:51:07.759329\n",
      "End of batch n°232 at 2018-11-13 14:53:14.142100\n",
      "Start batch n°233 at 2018-11-13 14:53:14.142573\n",
      "End of batch n°233 at 2018-11-13 14:55:03.716967\n",
      "Start batch n°234 at 2018-11-13 14:55:03.717123\n",
      "End of batch n°234 at 2018-11-13 14:56:45.299685\n",
      "Start batch n°235 at 2018-11-13 14:56:45.316732\n",
      "End of batch n°235 at 2018-11-13 14:59:41.752554\n",
      "Start batch n°236 at 2018-11-13 14:59:41.752991\n",
      "End of batch n°236 at 2018-11-13 15:01:41.657031\n",
      "End of training at: 2018-11-13 15:01:41.706711\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import datetime\n",
    "\n",
    "print('Start training at: {}'.format(datetime.datetime.now()))\n",
    "\n",
    "model = None\n",
    "count = 1\n",
    "for batch in batch_pipeline:\n",
    "    print('Start batch n°{} at {}'.format(count, datetime.datetime.now()))\n",
    "    if model is None:\n",
    "        model = Word2Vec(list(batch), size=300, sg=1, window=1, min_count=1, workers=16)\n",
    "    else:\n",
    "        try:\n",
    "            model.train(iter(batch), total_examples=len(batch), epochs=model.epochs)\n",
    "        except RuntimeError:\n",
    "            break\n",
    "    print('End of batch n°{} at {}'.format(count, datetime.datetime.now()))\n",
    "    count += 1\n",
    "\n",
    "print('End of training at: {}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file = open('trained_embeddings_23M.model', 'w+')\n",
    "model.save('trained_embeddings_23M.model')\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec.load('./trained_embeddings_23M.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('depress', 0.46674293279647827), ('weird', 0.4597640335559845), ('sick', 0.4553745985031128), ('disappoint', 0.45280617475509644), ('frustrat', 0.45277246832847595), ('upset', 0.4420504570007324), ('funni', 0.43968212604522705), ('happi', 0.4352729916572571), ('annoy', 0.4248059093952179), ('excit', 0.4106306731700897)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/home/nicolas/.pythonenv/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('sad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('believ', 0.4070039987564087), ('tell', 0.3571561276912689), ('respect', 0.356683611869812), ('faith', 0.3467700481414795), ('understand', 0.34412968158721924), ('let', 0.3375264108181), ('promis', 0.3068927526473999), ('control', 0.30342984199523926), ('bother', 0.3027229309082031), ('underestim', 0.30046722292900085)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/home/nicolas/.pythonenv/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('trust'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('appreci', 0.3969097435474396), ('support', 0.3940708637237549), ('confid', 0.36279332637786865), ('trust', 0.3566836714744568), ('employ', 0.3524959683418274), ('faith', 0.3501768112182617), ('digniti', 0.34686940908432007), ('encourag', 0.3448216915130615), ('love', 0.3382894694805145), ('admir', 0.33313828706741333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/home/nicolas/.pythonenv/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('respect'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('daddi', 0.4360034763813019), ('mama', 0.4000909626483917), ('girl', 0.3890559673309326), ('kid', 0.3793148696422577), ('princess', 0.370619535446167), ('boy', 0.3642154037952423), ('preciou', 0.35340815782546997), ('son', 0.3482373356819153), ('puppi', 0.3445145785808563), ('babe', 0.3434704840183258)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/home/nicolas/.pythonenv/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('babi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('amaz', 0.5798534154891968), ('awesom', 0.5109658241271973), ('unbeliev', 0.46301302313804626), ('insan', 0.4410589933395386), ('extrem', 0.3975587487220764), ('fantast', 0.394329309463501), ('phenomen', 0.390927791595459), ('outstand', 0.36891746520996094), ('aw', 0.36402973532676697), ('brilliant', 0.35805773735046387)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('incred'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dude', 0.566826581954956), ('woman', 0.5109279155731201), ('guy', 0.44352054595947266), ('boy', 0.4050363302230835), ('girl', 0.396494060754776), ('brother', 0.37180647253990173), (\"man'\", 0.3705722987651825), ('bro', 0.370300829410553), ('mother', 0.3333069980144501), ('son', 0.3330507278442383)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/home/nicolas/.pythonenv/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('love', 0.5302918553352356), ('ma', 0.3786517381668091), ('appreci', 0.3643260598182678), ('arrghhhh', 0.3245513439178467), ('<3', 0.32012438774108887), ('pzizz.com/affiliates.asp?id=1995', 0.31416797637939453), ('bro', 0.31183305382728577), ('e-a-g-l-e-', 0.31055912375450134), ('hate', 0.3082127869129181), ('xx', 0.30289795994758606)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/home/nicolas/.pythonenv/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('luv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('felt', 0.5829184055328369), ('feelin', 0.40771132707595825), ('smell', 0.3914710283279419), ('sound', 0.376400887966156), ('tast', 0.3494603633880615), ('think', 0.3464612364768982), ('behav', 0.34026259183883667), ('offkey', 0.3203328847885132), ('suitabili', 0.31692323088645935), ('know', 0.3084157109260559)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/home/nicolas/.pythonenv/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('feel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('truck', 0.49514272809028625), ('vehicl', 0.4850355088710785), ('bike', 0.4163765609264374), ('motorcycl', 0.3982599377632141), ('garag', 0.39300549030303955), ('bu', 0.37754032015800476), ('merced', 0.37616923451423645), ('boat', 0.36503323912620544), ('phone', 0.36238914728164673), ('bathroom', 0.35686442255973816)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('car'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fantast', 0.590543270111084), ('good', 0.5120762586593628), ('fab', 0.5019850134849548), ('brilliant', 0.4965236783027649), ('terrif', 0.47120505571365356), ('awesom', 0.4628204107284546), ('amaz', 0.4508037269115448), ('nice', 0.44725051522254944), ('fabul', 0.44397595524787903), ('excel', 0.4163700342178345)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/home/nicolas/.pythonenv/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('great'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('better', 0.582304060459137), ('easier', 0.42270427942276), ('worst', 0.4013630151748657), ('harder', 0.39631474018096924), ('stronger', 0.39296117424964905), ('bigger', 0.3910757303237915), ('hotter', 0.3763754963874817), ('bad', 0.3730286955833435), ('cooler', 0.36886197328567505), ('colder', 0.36526110768318176)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/home/nicolas/.pythonenv/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('wors'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('horribl', 0.650357723236084), ('bad', 0.4911038875579834), ('shitti', 0.421306312084198), ('disgust', 0.41420310735702515), ('great', 0.40791767835617065), ('worst', 0.40693503618240356), ('pathet', 0.37771177291870117), ('tragic', 0.36558565497398376), ('aw', 0.36078372597694397), ('sad', 0.3571690320968628)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/home/nicolas/.pythonenv/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('terribl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about emojis?\n",
    "\n",
    "This part still needs a little work in order to user emojis unicode encoding. For the moment, empjis are handled as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':-)', 0.6159348487854004), (';)', 0.5726117491722107), ('<3', 0.5300270318984985), (':D', 0.4842647314071655), ('!', 0.474102258682251), (';-)', 0.43480759859085083), ('xx', 0.4305958151817322), ('lol', 0.42179274559020996), (':(', 0.4141804575920105), ('hehe', 0.40834444761276245)]\n",
      "[(':-(', 0.5189728736877441), (':/', 0.5026484727859497), ('lol', 0.4185192883014679), (':)', 0.4141804575920105), ('ugh', 0.3916919529438019), ('haha', 0.3840927183628082), ('<3', 0.3624846935272217), (':D', 0.346662312746048), (';)', 0.3448978066444397), ('xx', 0.3353324234485626)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/home/nicolas/.pythonenv/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(':)'))\n",
    "print(model.wv.most_similar(':('))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download our trained models\n",
    "\n",
    "You can find the models we trained with aroud 23M tweets here:\n",
    "\n",
    "https://mega.nz/#!oUshxYoZ!pbA40Xzi_1kmZ68UhgDzu1rSytn67h6iYW4MKgoJqJQ\n",
    "\n",
    "https://mega.nz/#!ZN1h0QYC!JqeIN3DhjoRrBmO75Qmg8fI4w4jqvqZNGeXUiMl3I9M\n",
    "\n",
    "https://mega.nz/#!IB8BXQzC!K-hrA6r-A99b_t3g-qqBQdRK7RZV4rJHrBvkZtBvo2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [x] Handle @ and #\n",
    "- [ ] Handle emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
