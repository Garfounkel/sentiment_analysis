{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we present our training environments, explain the technical issues we faced, how we tackled them, and finally we launch a large scale training of the word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We tested different techniques to fetch a sufficient dataset of tweets:\n",
    " - Use the twitter streaming API with the Tweepy python wrapper\n",
    " - Use twitterscraper python module to scrap tweets directly from Twitter pages\n",
    " - Download tweets from different sources available on the WEB.\n",
    "\n",
    "Our constraints are the following:\n",
    " - We need more than 30 million tweets, so the fetch technique must be fast enough\n",
    " - We want to limit the number of truncated tweet (i.e. tweets finished by ... cutting a sentence). This is important because training the word2vec embeddings take into account the neighbors of each word.\n",
    " - The language must be english\n",
    "\n",
    "## Use twitter streaming API\n",
    "\n",
    "This technique has been proved to be too slow to be usable in this project. Only one stream can be launched by IP address. This stream can only fetch less than 5 tweets per seconds, and rely a lot on the query words used. There is no need to perform the calculation of the compute time, the constraints are too strong.\n",
    "\n",
    "## Use a twitter WEB scraper\n",
    "\n",
    "This technique is way faster than the previous one. Our script using this technique is able to fetch a stable rate of 20 tweets per seconds, by batches of 800 tweets. It could be improved to perform the fetch part and the database writes in parallel, roughly improving the performances by an expected 50% in the best case.\n",
    "\n",
    "Still, this technique is too slow. It would take 470h of compute time for one worker to fetch the entire dataset, which is more than the time we have for the project. Even considering the previous improvements and that each of the 3 group member could run one worker in parallel, this technique is too expensive in compute time. Moreover the CPU usage is high when using the scraper, forbidding any cloud deployment due to prohibitive cost.\n",
    "\n",
    "## Download a dataset\n",
    "\n",
    "This is the most efficient way to fetch tweets we could afford. There are still some issues with the quality of the dataset. The entire class spent a lot of time to find a large enough dataset matching our constraints. We managed to find what we needed at : https://archive.org/details/archiveteam-twitter-stream-2017-11\n",
    "\n",
    "One can find the little preprocessing we performed on the dataset before inserting into our mysql database in the file \"decompress_dataset.py\"\n",
    "\n",
    "## Ensuring tweet unicity\n",
    "\n",
    "As mentioned before, we used a mysql database to store our tweets temporarily. This has two main objectives :\n",
    " - Low memory usage\n",
    " - Low disk usage\n",
    " - Low cost unicity check\n",
    " - Quite performant data access\n",
    "\n",
    "The unicity of each tweets is checked using the unique tweet id.\n",
    "\n",
    "The only drawback of the usage of a local database is that only the member of the group possessing it can access it 24/24h. We plan to push the data in a large json file on S3 when the dataset is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming from/to the database\n",
    "\n",
    "The code to use the mysql database can be found in mysql_utils.py.\n",
    "\n",
    "By default it uses the mysql database configured on my personnal machine.\n",
    "\n",
    "Print the 10 first tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8951, 'looking for some deep cry for help in the song ease on down the road, but not finding it. it truly is a happy song. damn dorothy and toto ruining my fun.')\n(9594, 'updating the postmarks project..')\n(9606, \"my krissy behind it's fine all of the time.\")\n(9607, \"It would be impossible to surf Linda Mar with the short board, but it won't stop teh Stewie!\")\n(9618, 'wondering when my conversation will be light hearted again...')\n(9619, 'Havin a drink at the 500 club in the mission -- to the sound of... oooo the israelites ya')\n(9626, 'At the yacht club, talking to the bartenders about Wes')\n(9639, 'Just made up the Deadwood Drinking Game. ')\n(9644, \"Limon in the Mission is tart, cool and refreshing. Like ceviche? You'll like Limon. Try sauvignon blanc with as an accent :)\")\n(9645, 'Is that a software architect, enterprise architect, or the real-world kind?')\n"
     ]
    }
   ],
   "source": [
    "from mysql_utils import mysql_reader\n",
    "\n",
    "for tweet in mysql_reader(max=10):\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to push the tweet with id 9619 again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors during insertion: 0\nNumber of tweets in the input stream: 1\nNumber of effectively inserted tweets: 0\n"
     ]
    }
   ],
   "source": [
    "from mysql_utils import Tweet, mysql_sink       \n",
    "\n",
    "tweets = [Tweet(9619, 'Havin a drink at the 500 club in the mission -- to the sound of... oooo the israelites ya')]\n",
    "\n",
    "errors, inserted, stream_size = mysql_sink(iter(tweets))\n",
    "\n",
    "print('Errors during insertion: {}'.format(errors))\n",
    "print('Number of tweets in the input stream: {}'.format(stream_size))\n",
    "print('Number of effectively inserted tweets: {}'.format(inserted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline\n",
    "\n",
    "To simplify the implementation and modification of a preprocessing pipeline of tweets, we implemented some classes to model this pipeline. The base classes can be found in the file \"processing_pipeline.py\".\n",
    "\n",
    "In this section we'll focus on the standardisation part:\n",
    " - Tolenization\n",
    " - Lemmatisation\n",
    " - Stemming\n",
    "\n",
    "Our preprocessing wrappers are available under \"text_preprocessing.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['look', 'for', 'some', 'deep', 'cri', 'for', 'help', 'in', 'the', 'song', 'eas', 'on', 'down', 'the', 'road', ',', 'but', 'not', 'find', 'it', '.', 'it', 'truli', 'be', 'a', 'happi', 'song', '.', 'damn', 'dorothi', 'and', 'toto', 'ruin', 'my', 'fun', '.']\n['updat', 'the', 'postmark', 'project', '..']\n['my', 'krissi', 'behind', \"it'\", 'fine', 'all', 'of', 'the', 'time', '.']\n['it', 'would', 'be', 'imposs', 'to', 'surf', 'linda', 'mar', 'with', 'the', 'short', 'board', ',', 'but', 'it', \"won't\", 'stop', 'teh', 'stewi', '!']\n['wonder', 'when', 'my', 'convers', 'will', 'be', 'light', 'heart', 'again', '...']\n['havin', 'a', 'drink', 'at', 'the', '500', 'club', 'in', 'the', 'mission', '-', '-', 'to', 'the', 'sound', 'of', '...', 'oooo', 'the', 'israelit', 'ya']\n['at', 'the', 'yacht', 'club', ',', 'talk', 'to', 'the', 'bartend', 'about', 'we']\n['just', 'make', 'up', 'the', 'deadwood', 'drink', 'game', '.']\n['limon', 'in', 'the', 'mission', 'be', 'tart', ',', 'cool', 'and', 'refresh', '.', 'like', 'cevich', '?', \"you'll\", 'like', 'limon', '.', 'tri', 'sauvignon', 'blanc', 'with', 'as', 'an', 'accent', ':)']\n['be', 'that', 'a', 'softwar', 'architect', ',', 'enterpris', 'architect', ',', 'or', 'the', 'real-world', 'kind', '?']\n"
     ]
    }
   ],
   "source": [
    "from processing_pipeline import Pipeline\n",
    "from text_preprocessing import TweetTokenizer, NLTKStemmer, NLTKLemmatizer, CorpusWrapper\n",
    "\n",
    "preprocessor_factories = [\n",
    "    TweetTokenizer,\n",
    "    lambda tokens: CorpusWrapper(NLTKStemmer, tokens),\n",
    "    lambda tokens: CorpusWrapper(NLTKLemmatizer, tokens),\n",
    "]\n",
    "\n",
    "test_raw_tweet_stream = mysql_reader(max=10)\n",
    "test_tweet_text_stream = map(lambda tweet: tweet[1], test_raw_tweet_stream)\n",
    "\n",
    "pipeline = Pipeline(test_tweet_text_stream, preprocessor_factories)\n",
    "\n",
    "for val in pipeline:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large scale training\n",
    "\n",
    "In order to use gensim word2vec implementation, we need to provide batches of tokenized sentences. We use batches of 10000 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial                  # Nicer than lambdas\n",
    "from mysql_utils import MysqlTweetTextGetter   # Nicer than maps\n",
    "from processing_pipeline import BatchMaker\n",
    "\n",
    "input_stream = mysql_reader(max=30000)\n",
    "\n",
    "factories = [\n",
    "    MysqlTweetTextGetter,\n",
    "    TweetTokenizer,\n",
    "    partial(CorpusWrapper, NLTKStemmer),\n",
    "    partial(CorpusWrapper, NLTKLemmatizer),\n",
    "    BatchMaker,\n",
    "]\n",
    "\n",
    "batch_pipeline = Pipeline(input_stream, factories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of batch n°2 at 2018-11-01 15:00:36.784892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start batch n°3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of batch n°3 at 2018-11-01 15:00:52.441293\nStart batch n°4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of batch n°4 at 2018-11-01 15:00:57.936902\nEnd of training at: 2018-11-01 15:00:57.937164\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import datetime\n",
    "\n",
    "print('Start training at: {}'.format(datetime.datetime.now()))\n",
    "\n",
    "model = None\n",
    "for count, batch in enumerate(iter(batch_pipeline)):\n",
    "    print('Start batch n°{}'.format(count + 1))\n",
    "    if model is not None:\n",
    "        model.train(batch, total_examples=len(batch), epochs=model.epochs)\n",
    "    model = model or Word2Vec(batch, size=300, sg=1, window=1, min_count=1)\n",
    "    print('End of batch n°{} at {}'.format(count + 1, datetime.datetime.now()))\n",
    "\n",
    "print('End of training at: {}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
